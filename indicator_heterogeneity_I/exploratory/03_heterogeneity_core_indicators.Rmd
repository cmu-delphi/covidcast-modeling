---
title: "Indicator Heterogeneity I: Temporal Heterogeneity"
author: "Addison"
output:
  html_document
---
```{r import_statements, echo = FALSE, message = FALSE}
library(tidyverse)
library(covidcast)
```

```{r setup_multidplyr, echo = TRUE}
PARCOMPUTE = TRUE
N_CORE = parallel::detectCores()
```

# Background

In this notebook, we repeat the analysis of `02_temporal_heterogeneity.Rmd`
for all of our core indicators.

# Data setup

```{r data_ingestion_county, echo = TRUE, cache=TRUE}
# Fetch the following sources and signals from the API 
source_names = c("doctor-visits", "fb-survey", "fb-survey", "hospital-admissions")
signal_names = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_adj_covid19")
pretty_names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Hospitalizations")
geo_level = "county"

start_day = "2020-04-15"
end_day = NULL
cache_fname = 'cached_data/02_temporal_heterogeneity.RDS'

if (!file.exists(cache_fname)) {
  df_signals = vector("list", length(signals))
  for (i in 1:length(signals)) {
    df_signals[[i]] = suppressWarnings(
                        covidcast_signal(sources[i], signals[i],
                                         start_day, end_day,
                                         geo_type=geo_level))
  }

  # Fetch USAFacts confirmed case incidence proportion (smoothed with 7-day 
  # trailing average)
  df_cases = suppressWarnings(
              covidcast_signal("usa-facts", "confirmed_7dav_incidence_prop",
                              start_day, end_day,
                              geo_type=geo_level))

  case_num = 500
  geo_values = suppressWarnings(covidcast_signal("usa-facts", "confirmed_cumulative_num",
                                max(df_cases$time_value), 
                                max(df_cases$time_value))) %>%
    filter(value >= case_num) %>% pull(geo_value)
  saveRDS(list(df_signals, df_cases), cache_fname)
} else {
  cached_data = readRDS(cache_fname)
  df_signals = cached_data[[1]]
  df_cases = cached_data[[2]]
}
```

## Setup

```{r train_correlations, echo = TRUE}
sensorize_time_ranges = list(
      c(-14, -1),
      c(-14, -5),
      c(-21, -1),
      c(-21, -5))

for (ind_idx in 1:length(source_names)) {
  ind_df = tibble(df_signals[[ind_idx]])
  cases = tibble(df_cases)
  ind_cases = inner_join(ind_df, cases, by=c('geo_value', 'time_value')) %>% select (
        geo_value=geo_value,
        time_value=time_value,
        indicator_value=value.x,
        cases_value=value.y,
      )
	ind_global_sensorized =  ind_cases %>% group_by (
				geo_value,
			) %>% group_modify ( ~ {
				fit = lm(cases_value ~ indicator_value, data =.x);
				tibble(time_value=.x$time_value,
							 indicator_value=.x$indicator_value,
							 cases_value=.x$cases_value,
							 sensorized_value=fit$fitted.values)
			}) %>% ungroup
	df_global_sensorized = ind_global_sensorized %>% transmute (
				geo_value=geo_value,
				signal='ind_sensorized',
				time_value=time_value,
				direction=NA,
				issue=lubridate::ymd('2020-11-01'),
				lag=NA,
				value=sensorized_value,
				stderr=NA,
				sample_size=NA,
				data_source='linear_sensorization',
			)
	attributes(df_global_sensorized)$geo_type = 'county'
	attributes(df_global_sensorized)$metadata$geo_type = 'county'
	class(df_global_sensorized) = c("covidcast_signal", "data.frame")

	df_cor_base_ind = covidcast_cor(df_signals[[ind_idx]], df_cases,
																 by='time_value', method='spearman')
	df_cor_sensorized_ind = covidcast_cor(df_global_sensorized, df_cases,
																			 by='time_value', method='spearman')
	df_cor_base = rbind(df_cor_base_ind, df_cor_sensorized_ind)
	df_cor_base$Indicator = as.factor(c(rep('Raw', nrow(df_cor_base_ind)),
																      rep('Sensorized (Spatial)',
                                          nrow(df_cor_sensorized_ind))))


  sensorize_fname = sprintf('results/03_sensorize_cors_%s_%s.RDS',
                            source_names[ind_idx], signal_names[ind_idx])
  if (!file.exists(sensorize_fname)) {
    sensorize_cors = vector('list', length(sensorize_time_ranges))
    for (outer_idx in 1:length(sensorize_time_ranges)) {
      sensorize_llim = sensorize_time_ranges[[outer_idx]][1]
      sensorize_ulim = sensorize_time_ranges[[outer_idx]][2]

      min_sensorize_date = lubridate::ymd(start_day) - sensorize_llim
      max_sensorize_date = max(ind_cases$time_value)
      sensorize_date_offsets = 0:(max_sensorize_date-min_sensorize_date)

      joiner_df_list = vector('list', length(sensorize_date_offsets))
      for (idx in 1:length(sensorize_date_offsets)) {
        dt = sensorize_date_offsets[idx]
        sensorize_date = min_sensorize_date + dt
        joiner_df_list[[idx]] = tibble(
                          sensorize_date = sensorize_date,
                          time_value = sensorize_date + sensorize_llim:sensorize_ulim)
      }
      joiner_df = bind_rows(joiner_df_list)

      if (!PARCOMPUTE) {
        ind_sensorized_lm =  ind_cases %>% full_join(
              joiner_df,
              on='time_value',
            ) %>%  group_by (
              geo_value,
              sensorize_date,
            ) %>% group_modify (
              ~ broom::tidy(lm(cases_value ~ indicator_value, data = .x))
            ) %>% ungroup
      } else {
        ind_grouped_list =   ind_cases %>% full_join(
              joiner_df,
              on='time_value',
            ) %>%  group_by (
              geo_value,
              sensorize_date,
            ) %>% group_split
        ind_sensorized_lm = parallel::mclapply(ind_grouped_list, function(df) {
            broom::tidy(
              lm(cases_value ~ indicator_value, data = df)
            ) %>% mutate (
              geo_value = unique(df$geo_value),
              sensorize_date = unique(df$sensorize_date),
            )}, mc.cores = N_CORE) %>% bind_rows
      }
      ind_sensorized_wide = ind_sensorized_lm %>% select(
            geo_value,
            sensorize_date,
            term,
            estimate,
          ) %>% mutate (
            term = sapply(term, function(x) {ifelse(x=='(Intercept)',
                                                    'intercept',
                                                    'slope')}),
          ) %>% pivot_wider (
            id_cols = c(geo_value, sensorize_date),
            names_from=term,
            values_from=estimate,
          )
      ind_cases_sensorized = ind_cases %>% inner_join (
            ind_sensorized_wide,
            by=c('time_value'='sensorize_date',
                 'geo_value'),
          ) %>% mutate (
            sensorized_value = intercept + indicator_value * slope,
          )
      df_sensorized = ind_cases_sensorized %>% transmute (
            geo_value=geo_value,
            signal='ind_sensorized',
            time_value=time_value,
            direction=NA,
            issue=lubridate::ymd('2020-11-01'),
            lag=NA,
            value=sensorized_value,
            stderr=NA,
            sample_size=NA,
            data_source='linear_sensorization',
          )
      attributes(df_sensorized)$geo_type = 'county'
      class(df_sensorized) = c("covidcast_signal", "data.frame")

      df_cor_sensorized_ind = covidcast_cor(df_sensorized, df_cases,
                                           by='time_value', method='spearman')
      df_cor_sensorized_ind$Indicator = sprintf('Sensorized (TS, %d:%d)',
                                               sensorize_llim,
                                               sensorize_ulim)
      sensorize_cors[[outer_idx]] = df_cor_sensorized_ind
    }

    saveRDS(sensorize_cors, sensorize_fname)
  } else {
    sensorize_cors = readRDS(sensorize_fname)
  }

  df_cor = bind_rows(df_cor_base, sensorize_cors)
  plt = ggplot(df_cor, aes(x = time_value, y = value)) +
    geom_line(aes(color = Indicator)) +
    labs(title = sprintf("Correlation between %s and cases",
                         pretty_names[ind_idx]),
         subtitle = "Per day",
         x = "Date", y = "Correlation") +
    theme(legend.position = "bottom")
  print(plt)
}

```

