---
title: "Indicator Heterogeneity I: Temporal Heterogeneity"
author: "Addison"
output:
  html_document
---
```{r import_statements, echo = FALSE, message = FALSE}
library(tidyverse)
library(covidcast)
```

```{r setup_multidplyr, echo = TRUE}
PARCOMPUTE = TRUE
N_CORE = parallel::detectCores()
```

# Background

This notebook builds off `01_introduction.Rmd`, which introduced and motivated
the problem of heterogeneity, and then examined spatial heterogeneity in the DV
signal, ignoring the possibility of temporal heterogeneity (i.e., in the
_sensorization_ step, data was aggregated across all of time.  In this
notebook, we bring the temporal component under examination.

Recall that if the relationship between an indicator and its intended target
(for most, case rate; for the hospitalization indicator, death rate) is
temporally _homogeneous_ for a given location, then it does not evolve over
time.  Otherwise, it is considered temporally heterogeneous.  We have already
established in `01_introduction.Rmd` that DV, for example, is
_spatially_ heterogeneous.  We did this by "sensorizing" DV for each county;
i.e., we regressed case rate against DV and then took the fitted values as the
_sensorized_ DV indicator.  We saw that this spatial sensorization improved
the correlation of the indicator with case rate.

How can we quantify temporal heterogeneity?  We can perform a similar
sensorization, one in which we also consider "local windows of time", and
examine whether the correlation of the indicator with case rate.  A
sensorization that took into account time could be, for example, obtaining
the sensorized value for location j, day t, by regressing case rate against
the indicator using data in location j, days t-15 through t-1, and then 
fitting the value for location j, day t.  We could then see whether this
improves the correlation for the indicator beyond the raw indicator and
the (only) spatially sensorized indicator.

More broadly, however, we would like to be able to make statements about
if and when heterogeneity is present (in either the temporal or spatial
components).  In `01_introduction.Rmd`, for example, by producing a
chloropeth plot of fitted slope coefficients for different counties,
we were able to describe how the relationship between DV and case rate
differs geographically.  However, in order to quantifiably say there is
"enough" variance in the fitted slope coefficients to qualify as
"spatially heterogeneous", we would like some kind of scaling that "makes
sense"; or, even better, some kind of "null distribution" with which we
can describe how the data should look if the relationship is indeed
homogeneous.  This is what we will work towards in future notebooks.

# Data setup

```{r data_ingestion_county, echo = TRUE, cache=TRUE}
# Fetch the following sources and signals from the API 
sources = c("doctor-visits", "fb-survey", "fb-survey", "hospital-admissions")
signals = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_adj_covid19")
names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Hospitalizations")
geo_level = "county"

start_day = "2020-04-15"
end_day = NULL
cache_fname = 'cached_data/02_temporal_heterogeneity.RDS'

if (!file.exists(cache_fname)) {
  df_signals = vector("list", length(signals))
  for (i in 1:length(signals)) {
    df_signals[[i]] = suppressWarnings(
                        covidcast_signal(sources[i], signals[i],
                                         start_day, end_day,
                                         geo_type=geo_level))
  }

  # Fetch USAFacts confirmed case incidence proportion (smoothed with 7-day 
  # trailing average)
  df_cases = suppressWarnings(
              covidcast_signal("usa-facts", "confirmed_7dav_incidence_prop",
                              start_day, end_day,
                              geo_type=geo_level))

  case_num = 500
  geo_values = suppressWarnings(covidcast_signal("usa-facts", "confirmed_cumulative_num",
                                max(df_cases$time_value), 
                                max(df_cases$time_value))) %>%
    filter(value >= case_num) %>% pull(geo_value)
  saveRDS(list(df_signals, df_cases), cache_fname)
} else {
  cached_data = readRDS(cache_fname)
  df_signals = cached_data[[1]]
  df_cases = cached_data[[2]]
}
```

## Warmup: global correction

A global correction is easy to compute as a warmup.  Although it is not as
useful for understanding temporal nonstationarity, it can help us start to
understand spatial heterogeneity.  It will also help us develop a code
skeleton/framework for fancier analyses.

```{r temporal_spatial_sensorize, echo = TRUE}
sensorize_llim = -14
sensorize_ulim = -1

dv = tibble(df_signals[[1]])
cases = tibble(df_cases)
dv_cases = inner_join(dv, cases, by=c('geo_value', 'time_value')) %>% select (
      geo_value=geo_value,
      time_value=time_value,
      indicator_value=value.x,
      cases_value=value.y,
    )

min_sensorize_date = lubridate::ymd(start_day) - sensorize_llim
max_sensorize_date = max(dv_cases$time_value)
sensorize_date_offsets = 0:(max_sensorize_date-min_sensorize_date)

joiner_df_list = vector('list', length(sensorize_date_offsets))
for (idx in 1:length(sensorize_date_offsets)) {
  dt = sensorize_date_offsets[idx]
  sensorize_date = min_sensorize_date + dt
  joiner_df_list[[idx]] = tibble(
                    sensorize_date = sensorize_date,
                    time_value = sensorize_date + sensorize_llim:sensorize_ulim)
}
joiner_df = bind_rows(joiner_df_list)

dv_sensorized_lm_fname = 'cached_data/02_dv_sensorized_lm.RDS'
if (!file.exists(dv_sensorized_lm_fname)) {
  if (!PARCOMPUTE) {
    dv_sensorized_lm =  dv_cases %>% full_join(
          joiner_df,
          on='time_value',
        ) %>%  group_by (
          geo_value,
          sensorize_date,
        ) %>% group_modify (
          ~ broom::tidy(lm(cases_value ~ indicator_value, data = .x))
        ) %>% ungroup
  } else {
    dv_grouped_list =   dv_cases %>% full_join(
          joiner_df,
          on='time_value',
        ) %>%  group_by (
          geo_value,
          sensorize_date,
        ) %>% group_split
    dv_sensorized_lm_list = parallel::mclapply(dv_grouped_list, function(df) {
        broom::tidy(
          lm(cases_value ~ indicator_value, data = df)
        ) %>% mutate (
          geo_value = unique(df$geo_value),
          sensorize_date = unique(df$sensorize_date),
        )}, mc.cores = N_CORE)
    dv_sensorized_lm = bind_rows(dv_sensorized_lm_list)
  }
  saveRDS(dv_sensorized_lm, dv_sensorized_lm_fname)
} else {
  dv_sensorized_lm = readRDS(dv_sensorized_lm_fname)
}
dv_sensorized_wide = dv_sensorized_lm %>% select(
      geo_value,
      sensorize_date,
      term,
      estimate,
    ) %>% mutate (
      term = sapply(term, function(x) {ifelse(x=='(Intercept)',
                                              'intercept',
                                              'slope')}),
    ) %>% pivot_wider (
      id_cols = c(geo_value, sensorize_date),
      names_from=term,
      values_from=estimate,
    )
dv_cases_sensorized = dv_cases %>% inner_join (
      dv_sensorized_wide,
      by=c('time_value'='sensorize_date',
           'geo_value'),
    ) %>% mutate (
      sensorized_value = intercept + indicator_value * slope,
    )
```

```{r sensorized_histogram, echo = TRUE}
plot_lim =  100
outliers = dv_cases_sensorized %>% filter(
      abs(slope) > plot_lim,
    )
print(outliers)
plt = (ggplot (
      dv_cases_sensorized
    ) + geom_histogram (
      aes(slope),
      bins=200,
    ) + xlim (
      -plot_lim, plot_lim
    ) + ggtitle (
      sprintf('%d outliers not plotted', nrow(outliers))
    ))
plt
```

Surprisingly, there are quite a few negative slopes.  Noise?

```{r sensorized_correlation, echo = TRUE}
df_sensorized = dv_cases_sensorized %>% transmute (
      geo_value=geo_value,
      signal='dv_sensorized',
      time_value=time_value,
      direction=NA,
      issue=lubridate::ymd('2020-11-01'),
      lag=NA,
      value=sensorized_value,
      stderr=NA,
      sample_size=NA,
      data_source='linear_sensorization',
    )
attributes(df_sensorized)$geo_type = 'county'
class(df_sensorized) = c("covidcast_signal", "data.frame")

df_cor_sensorized_dv = covidcast_cor(df_sensorized, df_cases,
                                     by='time_value', method='spearman')
df_cor = readRDS('results/01_df_cor.RDS')
df_cor_sensorized_dv$Indicator = 'Sensorized (Temporal+Spatial)'
df_cor = bind_rows(df_cor, df_cor_sensorized_dv)
plt = ggplot(df_cor, aes(x = time_value, y = value)) +
  geom_line(aes(color = Indicator)) +
  labs(title = sprintf("Correlation between %s and cases", "DV"),
       subtitle = "Per day",
       x = "Date", y = "Correlation") +
  theme(legend.position = "bottom")
plt
```

This is, in a sense, nearly the "best case" for temporal sensorization: we use
all the data up until the day before sensorization (we might be able to improve
further by tuning how far in the past we look).  In practice, data often arrives
"late" and is backfilled.  We deal with an idealized form of the former problem
by only allowing ourselves to use DV data up until 5 days in the past in the
following exercise.  We treat the backfill issue in a later notebook by using
the `as_of` argument.

```{r train_correlations, echo = TRUE}
sensorize_time_ranges = list(
      c(-14, -1),
      c(-14, -5),
      c(-21, -1),
      c(-21, -5))
sensorize_cors = vector('list', length(sensorize_time_ranges))
sensorize_cors[[1]] = df_cor_sensorized_dv %>% mutate(Indicator='Sensorized (TS, -14:-1)')

for (outer_idx in 2:length(sensorize_time_ranges)) {
  sensorize_llim = sensorize_time_ranges[[outer_idx]][1]
  sensorize_ulim = sensorize_time_ranges[[outer_idx]][2]

  min_sensorize_date = lubridate::ymd(start_day) - sensorize_llim
  max_sensorize_date = max(dv_cases$time_value)
  sensorize_date_offsets = 0:(max_sensorize_date-min_sensorize_date)

  joiner_df_list = vector('list', length(sensorize_date_offsets))
  for (idx in 1:length(sensorize_date_offsets)) {
    dt = sensorize_date_offsets[idx]
    sensorize_date = min_sensorize_date + dt
    joiner_df_list[[idx]] = tibble(
                      sensorize_date = sensorize_date,
                      time_value = sensorize_date + sensorize_llim:sensorize_ulim)
  }
  joiner_df = bind_rows(joiner_df_list)

  if (!PARCOMPUTE) {
    dv_sensorized_lm =  dv_cases %>% full_join(
          joiner_df,
          on='time_value',
        ) %>%  group_by (
          geo_value,
          sensorize_date,
        ) %>% group_modify (
          ~ broom::tidy(lm(cases_value ~ indicator_value, data = .x))
        ) %>% ungroup
  } else {
    dv_grouped_list =   dv_cases %>% full_join(
          joiner_df,
          on='time_value',
        ) %>%  group_by (
          geo_value,
          sensorize_date,
        ) %>% group_split
    dv_sensorized_lm = parallel::mclapply(dv_grouped_list, function(df) {
        broom::tidy(
          lm(cases_value ~ indicator_value, data = df)
        ) %>% mutate (
          geo_value = unique(df$geo_value),
          sensorize_date = unique(df$sensorize_date),
        )}, mc.cores = N_CORE) %>% bind_rows
  }
  dv_sensorized_wide = dv_sensorized_lm %>% select(
        geo_value,
        sensorize_date,
        term,
        estimate,
      ) %>% mutate (
        term = sapply(term, function(x) {ifelse(x=='(Intercept)',
                                                'intercept',
                                                'slope')}),
      ) %>% pivot_wider (
        id_cols = c(geo_value, sensorize_date),
        names_from=term,
        values_from=estimate,
      )
  dv_cases_sensorized = dv_cases %>% inner_join (
        dv_sensorized_wide,
        by=c('time_value'='sensorize_date',
             'geo_value'),
      ) %>% mutate (
        sensorized_value = intercept + indicator_value * slope,
      )
  df_sensorized = dv_cases_sensorized %>% transmute (
        geo_value=geo_value,
        signal='dv_sensorized',
        time_value=time_value,
        direction=NA,
        issue=lubridate::ymd('2020-11-01'),
        lag=NA,
        value=sensorized_value,
        stderr=NA,
        sample_size=NA,
        data_source='linear_sensorization',
      )
  attributes(df_sensorized)$geo_type = 'county'
  class(df_sensorized) = c("covidcast_signal", "data.frame")

  df_cor_sensorized_dv = covidcast_cor(df_sensorized, df_cases,
                                       by='time_value', method='spearman')
  df_cor_sensorized_dv$Indicator = sprintf('Sensorized (TS, %d:%d)',
                                           sensorize_llim,
                                           sensorize_ulim)
  sensorize_cors[[outer_idx]] = df_cor_sensorized_dv
}

saveRDS(sensorize_cors, 'results/02_sensorize_cors.RDS')

df_cor = readRDS('results/01_df_cor.RDS')
df_cor = bind_rows(df_cor, sensorize_cors)
plt = ggplot(df_cor, aes(x = time_value, y = value)) +
  geom_line(aes(color = Indicator)) +
  labs(title = sprintf("Correlation between %s and cases", "DV"),
       subtitle = "Per day",
       x = "Date", y = "Correlation") +
  theme(legend.position = "bottom")
plt

```

