---
title: "Indicator Heterogeneity I: November 12th Report"
author: "Addison"
output:
  html_document:
    toc: true
    code_folding: hide
---
```{r import_statements, message = FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, cache=TRUE) 
library(tidyverse)
library(covidcast)
```

```{r data_setup, echo = TRUE}
# Fetch the following sources and signals from the API 
# TODO: Add Google Symptoms "eventually"
source_names = c("doctor-visits", "fb-survey", "fb-survey", "hospital-admissions")
signal_names = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_adj_covid19")
pretty_names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Hospitalizations")
target_names = c("Cases", "Cases", "Cases", "Deaths")
geo_level = "county"

start_day = "2020-04-15"
end_day = NULL
cache_fname = 'cached_data/03_heterogeneity_core_indicators.RDS'

if (!file.exists(cache_fname)) {
  df_signals = vector("list", length(signal_names))
  for (i in 1:length(signal_names)) {
    df_signals[[i]] = suppressWarnings(
                        covidcast_signal(source_names[i], signal_names[i],
                                         start_day, end_day,
                                         geo_type=geo_level))
  }

  # Fetch USAFacts confirmed case incidence proportion (smoothed with 7-day 
  # trailing average)
  df_cases = suppressWarnings(
              covidcast_signal("usa-facts", "confirmed_7dav_incidence_prop",
                              start_day, end_day,
                              geo_type=geo_level))

  df_deaths = suppressWarnings(
              covidcast_signal("usa-facts", "deaths_7dav_incidence_prop",
                              start_day, end_day,
                              geo_type=geo_level))

  case_num = 500
  geo_values = suppressWarnings(covidcast_signal("usa-facts", "confirmed_cumulative_num",
                                max(df_cases$time_value), 
                                max(df_cases$time_value))) %>%
    filter(value >= case_num) %>% pull(geo_value)
  saveRDS(list(df_signals, df_cases, df_deaths), cache_fname)
} else {
  cached_data = readRDS(cache_fname)
  df_signals = cached_data[[1]]
  df_cases = cached_data[[2]]
  df_deaths = cached_data[[3]]
}
```

```{r helper_functions, echo = TRUE}
plot_points = function(plt) {
  plt + geom_point(
    aes(
      x=indicator_value,
      y=target_value,
      color=time_value,
    ),
    alpha=ALPHA,
  ) + scale_colour_viridis_c(
    trans='date',
  ) + facet_wrap (
    vars(county_name_fips),
    nrow=nr,
    scales='free',
  )
}
```

# Introduction

The purpose of this note is to give a first look at heterogeneity (spatial and
temporal) relationships between our indicators and the quantities that we
wish to predict -- usually, case rates, or death rates.

## What is heterogeneity?

Heterogeneity in a relationship between two variables $x$ and $y$  means that
their relationship "varies".  For our purposes, this can mean that the
relationship varies in time (temporal heterogeneity) or it varies across
locations (spatial heterogeneity).

Aaron first [discovered](https://delphi-org.slack.com/archives/C011EU72MU2/p1599862022069500)
heterogeneity in the doctor visits indicator, which inspired this DAP.  The
idea of _sensorization_, which we apply in this DAP to account for
heterogeneity, has a long history in Delphi; see
[David's thesis](https://delphi.cmu.edu/~dfarrow/thesis.pdf) and
[Maria's paper](https://papers.nips.cc/paper/9475-kalman-filter-sensor-fusion-and-constrained-regression-equivalences-and-insights.pdf).

## Motivating examples

In the following plots, we see both _spatial heterogeneity_, in which
two counties exhibit the relationship between doctors visits and case
rate different; as well as _temporal heterogeneity_, where, within a 
single county, the relationship between doctors visits and case rate
evolves with time.

```{r build_example_plots, echo = TRUE}
dv = tibble(df_signals[[1]])
cases = tibble(df_cases)
county_tibble = covidcast::county_geo %>% transmute (
      geo_value = fips,
      county_name=county,
      state=abbr,
      county_name_fips = sprintf('FIPS: %s\n%s, %s',
                                 geo_value, county_name, state),
    )
dv_cases = inner_join(
      df_signals[[1]], cases, by=c('geo_value', 'time_value')
    ) %>% select (
      geo_value=geo_value,
      time_value=time_value,
      indicator_value=value.x,
      target_value=value.y,
    ) %>% inner_join (
			county_tibble,
			on='geo_value',
    ) %>% tibble


MARICOPA_AZ = '04013'
FRANKLIN_OH = '39049'
FULTON_IL   = '17057'
NATRONA_WY  = '56025'
BROWN_SD    = '46013'

TEMP_HET = MARICOPA_AZ
SPAT_HET = c(FRANKLIN_OH,
#             NATRONA_WY,
             BROWN_SD)


plt_temporal_heterogeneity = dv_cases %>% filter (
      geo_value == TEMP_HET,
    ) %>% ggplot(
    ) + geom_point(
      shape=21,
      colour='black',
      aes(
        x=indicator_value,
        y=target_value,
        fill=time_value,
      ),
    ) + scale_fill_viridis_c(
      trans='date',
    ) + xlab (
      "Doctor visits"
    ) + ylab (
      "Cases per 100k"
    ) + ggtitle (
      "Temporal heterogenenity (Maricopa County, AZ)"
    ) + theme(legend.position = "bottom"
    )

plt_spatial_heterogeneity = dv_cases %>% filter (
      geo_value %in% SPAT_HET
    ) %>% ggplot(
    ) + geom_point(
      shape=21,
      colour='black',
      aes(
        x=indicator_value,
        y=target_value,
        #fill=geo_value,
        fill=county_name_fips,
      ),
    ) + xlab (
      "Doctor visits"
    ) + ylab (
      "Cases per 100k"
    ) + ggtitle (
      "Spatial heterogeneity"
    ) + theme(legend.position = "bottom"
    )

```

```{r render_example_plots, echo = TRUE, fig.width=10, fig.height=5}
gridExtra::grid.arrange(
                        plt_spatial_heterogeneity,
                        plt_temporal_heterogeneity,
                        ncol=2)

```

# Methods

First, we fix notation.  Assume an indicator and target (e.g., doctors visits
and case rate), which we suppress notationally for brevity.  Each observation
is then represented as $(x_{t\ell}, y_{t\ell})$, where $x$ is the indicator
value, $y$ is the target value, $t$ represents time (measured in dates),
and $\ell$ represents location.  Let $L$ denote the set of all valid locations,
e.g., all counties.  Let $x_{t\cdot}$ denote all the $x_{t\ell}$ collected
across locations in $L$, and similarly for $y_{t\cdot}$.  Let
$x_{t_1:t_2, \ell}$, $y_{t_1:t_2, \ell}$ denote the observations that fall
within times $t_1, t_2$, endpoints included.  Finally, let $x, y$
be the collection of all observations across time and location.

In the classical, unsensorized approach, we take the $x_{t\ell}$ and
hope that they give some indication of the intensity of $y_{t\ell}$, e.g.,
we may compute the Spearman correlation between $x_{t\cdot}$ and
$y_{t\cdot}$ for each $t$:

```{r plot_example_correlation, echo = TRUE}
ind_idx=1
base_cor_fname = sprintf('results/03_base_cors_%s_%s.RDS',
                          source_names[ind_idx], signal_names[ind_idx])
df_cor_base = readRDS(base_cor_fname) %>% filter (
      Indicator == 'Raw'
    )

plt = ggplot(df_cor_base, aes(x = time_value, y = value)) +
  geom_line(
            #aes(color = Indicator)
            ) +
  labs(title = sprintf("Correlation between %s and %s",
                       pretty_names[ind_idx],
                       target_names[ind_idx]),
       subtitle = "Per day",
       x = "Date", y = "Correlation") +
  theme(legend.position = "bottom")
print(plt)
```

For some reason, $x$ by itself is "not great" at giving us a sense
of how $y$ is doing.  The idea behind _sensorization_ is to replace
$x$ with $\tilde x$ which does better.  We do this by learning specific
relationships between $x$ and $y$ for each location and time window
and using these relationships to "correct" x into $\tilde x$.

## Spatial-only sensorization

In the basic, spatial-only form of sensorization (as computed in Aaron's
notebook), we ignore the possibility of temporal heterogeneity and learn
a single linear relationship between the indicator and target for
each location.  Specifically, we learn, for each $\ell \in L$
$$
y_{\cdot\ell} \sim x_{\cdot\ell} \qquad\Rightarrow \mathrm{Model}(\ell)
$$
and obtain the sensorized indicator values
$$
\tilde x_{t\ell} = \texttt{predict}(x_{t\ell}, \mathrm{Model}(\ell)) = \hat y_{t\ell}.
$$
Note, importantly, that the sensorized $\tilde x$ is no longer on the same
scale as the original indicator $x$.

## Temporal and spatial sensorization (TS)

Let $k$ denote the number of days into the past we wish to examine data
when fitting our model.  (Smaller $k$ is "more adaptive" to temporal
heterogeneity, but may also lead to less stable fits).

We fit, for each $t, \ell$:
$$
y_{(t-k):t, \ell} \sim x_{(t-k):t, \ell} \qquad\Rightarrow \mathrm{Model}(\ell, t, k)
$$
and obtain the sensorized indicator values
$$
\tilde x_{t\ell} = \texttt{predict}(x_{t\ell}, \mathrm{Model}(\ell, t, k))
$$
We fit a linear model for each location and day, and then take the prediction
for that day.  (Huge number of models to fit, but embarrassingly
parallelizable).

## Temporal and spatial sensorization, accounting for delay (TS Delayed)

A wrinkle to our approach is added when we consider the problems of
_data delay_ and _backfill_.  For some indicators, data is available
"immediately" (usually, this means the morining after the day to be measured)
and that version of the data is "final".  This is the ideal case.

However, for many indicators, data is not available until a few days after
the date to be measured; and once it becomes available, the data for that
date is subsequently updated for several days before it becomes "finalized".
This has bearing on our sensorization analysis, because the historical
correlations we obtain by sensorizing using the latest available data can be
different from the correlations obtained by sensorizing with contemporaneous
data.

We formalize the fact that the data for a single time $t$ has several versions
using a superscript.  Let us denote by $x_{t\ell}^{(t')}$ the indicator
value for time $t$ and location $\ell$, as reported at time $t'$.  (Similarly,
apply this notation for $y$; and inherit the subscript notation from earlier).
When we say $x_{t\ell}^{(\infty)}$, we mean "the latest available data at
time of analysis".  Hence, all methods up until now have assumed $t' = \infty$.

Recalling $k$ as the number of days into the past we wish to examine
data when fitting our model, and fixing $\Delta$ to be the "delay" at which
we sensorize, we fit
$$
y_{(t-k):t, \ell}^{(t+\Delta)} \sim x_{(t-k):t, \ell}^{(t+\Delta)}
    \qquad\Rightarrow \mathrm{Model}(\ell, t, k, t+\Delta)
$$
and obtain the sensorized indicator values
$$
\tilde x_{t\ell} = \texttt{predict}(x_{t\ell}, \mathrm{Model}(\ell, t, k, t+\Delta))
$$
Fitting this model carries the computational annoyance of having to form
a new dataset for each $t$, a task whose burden is greatly lessened by the
`as_of` functionality in our COVIDcast API.

# Results

Here we report results for the doctors visits, Facebook %CLI, Facebook
%CLI-in-community, and hospitalization indicators.  For the first three
indicators, the target is cases per 100k; for hospitalization, the target is
deaths per 100k.

We compute correlations using the raw (unsensorized) indicator;
spatial sensorized, spatial and temporal sensorized.  For the latter
sensorization, we use several window sizes: up to 7, 10, 14, and 21 days
into the past.

In broad terms, we find that sensorizing for both time and space provides
the best results; as does using a smaller time window.

```{r train_correlations, echo = TRUE, fig.width=10, fig.height=5}
sensorize_time_ranges = list(
      c(-7, 0),
      c(-10, 0),
      c(-14, 0),
      c(-21, 0))

for (ind_idx in 1:length(source_names)) {
  base_cor_fname = sprintf('results/03_base_cors_%s_%s.RDS',
                            source_names[ind_idx], signal_names[ind_idx])
  df_cor_base = readRDS(base_cor_fname)
  sensorize_fname = sprintf('results/03_sensorize_cors_%s_%s.RDS',
                            source_names[ind_idx], signal_names[ind_idx])
  sensorize_cors = readRDS(sensorize_fname)

  df_cor = bind_rows(df_cor_base, sensorize_cors)
  df_cor$Indicator = factor(df_cor$Indicator,
                            levels=c('Raw',
                                     'Sensorized (Spatial)',
                                     sapply(sensorize_time_ranges,
                                            function(x) {
                                              sprintf('Sensorized (TS, %d:%d)',
                                                      x[[1]], x[[2]])
                                            })))

  plt = ggplot(df_cor, aes(x = time_value, y = value)) +
    geom_line(aes(color = Indicator)) +
    labs(title = sprintf("Correlation between %s and %s",
                         pretty_names[ind_idx],
                         target_names[ind_idx]),
         subtitle = "Per day",
         x = "Date", y = "Correlation") +
    theme(legend.position = "bottom")
  print(plt)
}
```

# Discussion & next steps

## TS Delayed model

## Quantifying degree of heterogeneity
