---
title: "Indicator Heterogeneity I: December 7th Report"
author: "Addison"
output:
  html_document:
    toc: true
    code_folding: hide
    fig_width: 12
    fig_height: 7
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(covidcast)

source_names = c("doctor-visits", "fb-survey", "fb-survey",
                 "hospital-admissions", "hospital-admissions")
signal_names = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_adj_covid19_from_claims", "smoothed_adj_covid19_from_claims")
pretty_names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Hospitalizations", "Hospitalizations")
target_names = c("Cases", "Cases", "Cases", "Cases", "Deaths")
geo_level = 'county'
sensor_target_names = sapply(1:5, function(ind_idx) {
  sprintf('%s // Sensorization target: %s', pretty_names[ind_idx], target_names[ind_idx])
         })
```

# Introduction

TODO intro

## What do we want from sensorization?

### Improved ability to interpret an indicator across locations

* **Approach.** Adjust relative scale of each indicator so that location-wise
  comparisons are meaningful.  (This is not automatically satisfied by the
  raw indicators; e.g.,
  [Aaron's blog post](https://delphi.cmu.edu/blog/2020/11/05/a-syndromic-covid-19-indicator-based-on-insurance-claims-of-outpatient-visits/)
  shows that the DV indicator exhibits different base rates in different
  HHS regions).  In essence, we want to correct _spatial heterogeneity_.
* **Measurement.**  Quantified by measuring (improvement in) geo-wise
  correlation of cases (rank correlations after grouping by time $t$).
* Note that it can be dangerous to absentmindedly maximize this quantity
  against the target, because then it is trivially maximized by taking the 
  target signal.  (We attempt to avoid this by using large sensorization 
  time windows, and by setting up an validation exercise against a
  new, unseen target).

TODO: also show correction re: hhs

### Improved ability to interpret an indicator within a location

* This problem is essentially that of _temporal heterogeneity_.  I believe 
  that it is more subtle and more difficult to solve.
* If the target "trustworthy", i.e., it is not subject to reporting changes
  or degradation, for example, then sensorization should account for this, 
  and we would be able to measure it through timewise correlation.
* However, I believe that the unreliability of the cases indicator
  [requires justification] make it difficult for us to properly learn a
  temporal adjustment [dicuss].  [Might be a good place to discuss 
  the step change in intercept for the many indicators.]

### Improved predictive ability for case rates

This is measured through the "simple forecaster" setup that
[Ryan has built](https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/).

### Interpretability via an inverse mapping

Deferred to future work (I believe Aaron is working on this).

# Proposed sensorization scheme

First, we fix notation.  Assume an indicator and target (e.g., doctors visits
and case rate), which we suppress notationally for brevity.  Each observation
is then represented as $(x_{t\ell}, y_{t\ell})$, where $x$ is the indicator
value, $y$ is the target value, $t$ represents time (measured in dates),
and $\ell$ represents location.  Let $L$ denote the set of all valid locations,
e.g., all counties.  Let $x_{t\cdot}$ denote all the $x_{t\ell}$ collected
across locations in $L$, and similarly for $y_{t\cdot}$.  Let
$x_{t_1:t_2, \ell}$, $y_{t_1:t_2, \ell}$ denote the observations that fall
within times $t_1, t_2$, endpoints included.  Finally, let $x, y$
be the collection of all observations across time and location.

Let $k_1, k_2$ denote the number of days into the past we wish to examine data
when fitting our model.  (Smaller $k$ is "more adaptive" to temporal
heterogeneity, but may also lead to less stable fits).

We fit, for each $t, \ell$:
$$
y_{(t-k):(t-1), \ell} \sim x_{(t-k_1):(t-k_2), \ell} \qquad\Rightarrow
\mathrm{Model}(\ell, t, k_1, k_2)
$$
and obtain the sensorized indicator values
$$
\tilde x_{t\ell} = \texttt{predict}(x_{t\ell}, \mathrm{Model}(\ell, t, k_1, k_2))
$$
We fit a linear model for each location and day, and then take the prediction
for that day.  (Huge number of models to fit, but embarrassingly
parallelizable).

For our purposes we take $k_1, k_2$ to be $-70, -8$.  That is, we train over
a nine-week window, without the immediately preceding week of data.

# Performance of the sensorization scheme

### Geo-wise correlations

```{r geowise-correlations, cache=TRUE, warning=FALSE}
sensorize_time_ranges = list(
      c(-42, -8),
      c(-49, -8),
      c(-56, -8),
      c(-63, -8),
      c(-70, -8)
)
splot_idx = 5

df_cor_list = vector('list', length(source_names))
for (ind_idx in 1:length(source_names)) {
  base_cor_fname = sprintf('results/12_base_cors_%s_%s_%s_%s.RDS',
                           geo_level,
                           source_names[ind_idx], signal_names[ind_idx],
                           target_names[ind_idx])
  df_cor_base = readRDS(base_cor_fname)
  sensorize_fname = sprintf('results/12_sensorize_cors_%s_%s_%s_%s.RDS',
                            geo_level,
                            source_names[ind_idx], signal_names[ind_idx],
                            target_names[ind_idx])
  sensorize_val_fname = sprintf('results/12_sensorize_vals_%s_%s_%s_%s.RDS',
                            geo_level,
                            source_names[ind_idx], signal_names[ind_idx],
                            target_names[ind_idx])
  sensorize_cors = readRDS(sensorize_fname)[[splot_idx]]

  df_cor = bind_rows(df_cor_base, sensorize_cors)
  static_description = "Static Sensorization (Spatial)"
  dynamic_description = "Dynamic Sensorization (-70:-8)"
  df_cor$Indicator = sapply(df_cor$Indicator, function(x) {
      if (x == 'Sensorized (Spatial)') {
        return(static_description)
      } else if (x == sprintf('Sensorized (TS, %d:%d)',
                       sensorize_time_ranges[[splot_idx]][1],
                       sensorize_time_ranges[[splot_idx]][2])) {
        return(dynamic_description)
      } else { return(x) }})
  df_cor$Indicator = factor(df_cor$Indicator,
                            levels=c('Raw',
                                     static_description,
                                     dynamic_description))
  df_cor$sensor_target = sensor_target_names[ind_idx]
  df_cor_list[[ind_idx]] = df_cor
}
df_cor = bind_rows(df_cor_list)
df_cor$sensor_target = factor(
    df_cor$sensor_target,
    levels=sensor_target_names)

plt = ggplot(
    df_cor,
    aes(x = time_value,
        y = value,
        color=Indicator)
  ) + geom_line(
  ) + facet_wrap (
    vars(sensor_target),
    ncol=2,
  ) + theme_bw (
  ) + theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) + labs (
    x = 'Date',
    y = 'Rank correlation (geo-wise)'
  )
print(plt)

```

(TODO)

### Time-wise correlations

```{r timewise-correlations, cache=TRUE}
# TODO
# TODO: write a new script taht computes timewise correlations 
#       for the final, chosen indicator
timewise_cors_list = vector('list', length(source_names))
for (ind_idx in 1:length(source_names)) {
  timewise_cor_fname = sprintf('results/13_timewise_cors_%s_%s_%s_%s.RDS',
                               geo_level, source_names[ind_idx],
                               signal_names[ind_idx], target_names[ind_idx])
  timewise_cor_list = readRDS(timewise_cor_fname)
  timewise_cors = timewise_cor_list[[1]]
  timewise_cors_summarized = timewise_cor_list[[2]]
  min_dynamic_correlate = timewise_cors %>% filter (
      sensorization == 'dynamic',
    ) %>% pull (
      correlate_date,
    ) %>% min
  timewise_cors_summarized = timewise_cors_summarized %>% filter (
      sensorization %in% c('raw', 'static') |
        correlate_date >= min_dynamic_correlate+4,
    )
  timewise_cors_summarized$sensor_target = sensor_target_names[ind_idx]
  timewise_cors_list[[ind_idx]] = timewise_cors_summarized
}
timewise_cors_summarized = bind_rows(timewise_cors_list)
timewise_cors_summarized$sensor_target = factor(
    timewise_cors_summarized$sensor_target,
    levels = sensor_target_names)
timewise_cors_summarized$sensorization = factor(
    timewise_cors_summarized$sensorization,
    levels=c('raw', 'static', 'dynamic'))
plt = ggplot(
    timewise_cors_summarized,
    aes(x=correlate_date,
        colour=sensorization),
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=max,
        linetype='min/max'),
  ) + geom_line (
    aes(y=min,
        linetype='min/max'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed",
               "min/max"="dotted"),
      breaks=c('median',
               'med±mad',
               'min/max'),
  ) + facet_wrap (
    vars(sensor_target),
    ncol=2,
  ) + theme_bw (
  ) + theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) + labs (
    x='Date',
    y='Rank correlation (time-wise, over trailing 6 weeks)'
  )
print(plt)
```

(TODO)

### Predictive ability

```{r predictive-ability, cache=TRUE, warning=FALSE, message=FALSE}
sensor_target_names = sapply(1:5, function(ind_idx) {
  sprintf('%s\nTarget: %s', pretty_names[ind_idx], target_names[ind_idx])
         })
model_names = c('Targets',
                'Targets+Raw',
                'Targets+Static',
                'Targets+Dynamic')

lags = 1:2 * -7 
leads = 1:2 * 7

plot_df_list = vector('list', length(source_names))
for (ind_idx in 1:length(source_names)) {
  predictive_fname = sprintf('results/14_predictive_%s_%s_%s_%s.RDS', geo_level,
                               source_names[ind_idx], signal_names[ind_idx],
                               target_names[ind_idx])

  res = readRDS(predictive_fname)


  # Restrict to common period for all 4 models, then calculate the scaled errors 
  # for each model, that is, the error relative to the strawman's error
  res_all4 = res %>%
  #res_all4 = res %>% filter(geo_value %in% geo_values) %>%
    drop_na() %>%                                       # Restrict to common time
    mutate(err1 = err1 / err0, err2 = err2 / err0,      # Compute relative error
           err3 = err3 / err0, err4 = err4 / err0) %>%  # to strawman model
    mutate(dif12 = err1 - err2, dif13 = err1 - err3,    # Compute differences
           dif14 = err1 - err4) %>%                     # relative to cases model
    ungroup() %>%
    select(-err0) 
           
  # Calculate and print median errors, for all 4 models, and just 7 days ahead
  if (ind_idx > 1) {res_all4 = rename(res_all4, lead=lead_)}
  res_err4 = res_all4 %>% 
    select(-starts_with("dif")) %>%
    pivot_longer(names_to = "model", values_to = "err",
                 cols = -c(geo_value, time_value, lead)) %>%
    mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
           model = factor(model, labels = model_names))

  plot_df = res_err4 %>% group_by(
      model, lead, time_value
    ) %>% summarize(
      med = median(err),
      mad = mad(err),
      min = min(err),
      max = max(err),
    ) %>% ungroup()
  plot_df$sensor_target = sensor_target_names[ind_idx]
  plot_df_list[[ind_idx]] = plot_df
}
plot_df = bind_rows(plot_df_list)

plt = ggplot(
    plot_df,
    aes(x=time_value,
        colour=model)
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed"),
      breaks=c('median', "med±mad")
	) + scale_color_manual(
		values = c("black",
               '#F8766D',
               '#00BA38',
               '#619CFF')
  ) + geom_hline(
    yintercept = 1,
    linetype = 2,
    color = "gray"
  ) + facet_grid(
    cols=vars(lead),
    rows=vars(sensor_target),
    scales='free',
  ) + ylim (
    0, 2
  ) + labs(
    x = "Date",
    y = "Scaled error"
  ) + theme_bw (
  ) + theme(
    legend.pos = "bottom",
    legend.title = element_blank()
  )
print(plt)
```

(TODO)

### Validation against hospitalization data

```{r hospitalization-validation}
hosp_cor_df = readRDS('results/15_hhs_cor_df.RDS')
hosp_cor_df$correlate_target = stringr::str_replace(
            hosp_cor_df$correlate_target,
            ': ',
            '\n')
plt = ggplot(
    hosp_cor_df,
    aes(x=time_value,
        y=value,
        colour=sensorization)
  ) + geom_line (
  ) + facet_grid (
    rows = vars(correlate_target),
    cols = vars(sensor_target),
  ) + theme_bw (
  ) + theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) + labs (
    x='Date',
    y='Rank correlation (geo-wise)'
  )
print(plt)
```

### Coefficients

As an extra "result", we illustrate the coefficients fitted by the
dynamic sensorization method, as a function of time.  

TODO also talk a bit about coefficients?
```{r coefficients_analysis, cache=TRUE, message=FALSE, warning=FALSE}
df_coef_list = vector('list', length(source_names))
for (ind_idx in 1:length(source_names)) {
  sensorize_val_fname = sprintf('results/12_sensorize_vals_%s_%s_%s_%s.RDS',
                            geo_level,
                            source_names[ind_idx], signal_names[ind_idx],
                            target_names[ind_idx])
  sensorize_vals = readRDS(sensorize_val_fname)[[splot_idx]]
  df_coef = sensorize_vals %>% select(
      geo_value,
      time_value,
      intercept,
      slope,
    )
  df_coef$sensor_target = sensor_target_names[ind_idx]
  df_coef_list[[ind_idx]] = df_coef
}
coef_df = bind_rows(df_coef_list) %>% pivot_longer (
    cols = c(slope, intercept),
    names_to = 'coefficient',
    values_to = 'value',
  )
coef_df_summarized = coef_df %>% group_by (
    time_value,
    sensor_target,
    coefficient,
  ) %>% summarize (
    med = median(value, na.rm=TRUE),
    mad = mad(value, na.rm=TRUE),
  ) %>% ungroup

plt = ggplot(
    coef_df_summarized,
    aes(x=time_value),
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed",
               "min/max"="dotted"),
      breaks=c('median',
               'med±mad',
               'min/max'),
  ) + facet_grid (
    rows=vars(sensor_target),
    cols=vars(coefficient),
    scale='free_y',
  )

plt_intercept = ggplot(
    coef_df_summarized %>% filter (coefficient=='intercept'),
    aes(x=time_value),
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed",
               "min/max"="dotted"),
      breaks=c('median',
               'med±mad',
               'min/max'),
  ) + facet_wrap (
    vars(sensor_target),
    ncol=1,
    scale='free_y',
  ) + labs (
    y = 'Intercept'
  ) + theme_bw (
  ) + theme(
    axis.title.x=element_blank(),
    legend.position = "bottom",
    legend.title = element_blank()
  )

plt_slope = ggplot(
    coef_df_summarized %>% filter (coefficient=='slope'),
    aes(x=time_value),
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed",
               "min/max"="dotted"),
      breaks=c('median',
               'med±mad',
               'min/max'),
  ) + facet_wrap (
    vars(sensor_target),
    ncol=1,
    scale='free_y',
  ) + labs (
    y = 'Slope'
  ) + theme_bw (
  ) + theme(
    axis.title.x=element_blank(),
    legend.position = "bottom",
    legend.title = element_blank()
  )

gridExtra::grid.arrange(plt_intercept, plt_slope, ncol=2)
plt_intercept2 = ggplot(
    coef_df_summarized %>% filter (coefficient=='intercept'),
    aes(x=time_value),
  ) + geom_line (
    aes(y=med,
        linetype='median'),
  ) + geom_line (
    aes(y=med+mad,
        linetype='med±mad'),
  ) + geom_line (
    aes(y=med-mad,
        linetype='med±mad'),
  ) + scale_linetype_manual(
      values=c("median"="solid",
               "med±mad"="dashed",
               "min/max"="dotted"),
      breaks=c('median',
               'med±mad',
               'min/max'),
  ) + facet_wrap (
    vars(sensor_target),
    scale='free_y',
  )
    
    

```


# Discussion
TODO
blah

