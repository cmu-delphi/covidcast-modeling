---
title: "Predictive Power Comparison"
author: "Jingjing"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, cache = TRUE, warning = FALSE)
```


```{r import_statements, message = FALSE}
library(tidyverse)
library(covidcast)
library(dplyr)
library(ggplot2)

# Use quantgen for LAD regression (this package supports quantile regression and
# more; you can find it on GitHub here: https://github.com/ryantibs/quantgen)
library(quantgen) 
library(gurobi)
```

```{r}
# Some useful functions for transformations
Log = function(x, a = 0.01) log(x + a)
Exp = function(y, a = 0.01) exp(y) - a
Logit = function(x, a = 0.01) log((x + a) / (1 - x + a))
Sigmd = function(y, a = 0.01) (exp(y) * (1 + a) - a) / (1 + exp(y))
Id = function(x) x
 
# Transforms to consider, in what follows
trans = Id
inv_trans = Id

# Set how far to forecast ahead.
lags = 1:2 * -7 
leads = 1:2 * 7
```

## County Level Analysis
### Getting data
```{r, echo = FALSE, message = FALSE, warning=FALSE, cache=TRUE}
# Fetch the following sources and signals from the API 
sources = c("doctor-visits", "fb-survey", "fb-survey", "hospital-admissions")
signals = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_adj_covid19")
names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Hospitalizations")

start_day = "2020-04-15"
end_day = "2020-10-15"

# Fetch USAFacts confirmed case incidence proportion (smoothed with 7-day 
# trailing average)
df_cases = covidcast_signal("usa-facts", "confirmed_7dav_incidence_prop",
                            start_day, end_day)

# Get geo values that appear in all datasets.
geo_values_complete = df_cases$geo_value
df_signals = vector("list", length(signals))
for (i in 1:length(signals)) {
  df_signals[[i]] = covidcast_signal(sources[i], signals[i], start_day, end_day)
  geo_values_complete = intersect(geo_values_complete, df_signals[[i]]$geo_value)
}


n = length(signals) + 3

library(stringr)
dir = "../test_files/google-symptoms/"
df_signals[[n-2]] = read.csv(paste(dir,"county_ageusia_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date"))  %>%
                                    filter(time_value >= as.Date(start_day) 
                                           &  time_value <= as.Date(end_day))
df_signals[[n-2]]$geo_value = str_pad(df_signals[[n-2]]$geo_value, width=5, side="left", pad="0")
sources[n-2] = "google-symptoms"
signals[n-2] = "ageusia_smoothed_search"
names[n-2] = "GS Ageusia Smoothed Search"

df_signals[[n-1]] = read.csv(paste(dir,"county_anosmia_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date")) %>%
                                    filter(time_value >= as.Date(start_day)
                                           &  time_value <= as.Date(end_day))
df_signals[[n-1]]$geo_value = str_pad(df_signals[[n-1]]$geo_value, width=5, side="left", pad="0")
sources[n-1] = "google-symptoms"
signals[n-1] = "anosmia_smoothed_search"
names[n-1] = "GS Anosmia Smoothed Search"

df_signals[[n]] = read.csv(paste(dir,"county_combined_symptoms_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date"))  %>%
                                    filter(time_value >= as.Date(start_day)
                                           &  time_value <= as.Date(end_day))
df_signals[[n]]$geo_value = str_pad(df_signals[[n]]$geo_value, width=5, side="left", pad="0")
sources[n] = "google-symptoms"
signals[n] = "combined_symptoms_smoothed_search"
names[n] = "GS A+A Smoothed Search"
```


```{r, warning=FALSE}
# Consider only counties with at least 200 cumulative cases
case_num = 200
geo_values = covidcast_signal("usa-facts", "confirmed_cumulative_num",
                              max(df_cases$time_value), 
                              max(df_cases$time_value), 
                              geo_type = "msa") %>%
  filter(value >= case_num) %>% pull(geo_value)

geo_values = intersect(geo_values, geo_values_complete)
```


### Prediction Preparation
Here we use code liberally borrowed from a
[blog post](https://github.com/cmu-delphi/delphi-blog/blob/main/content/post/2020-09-21-forecast-demo.Rmd)
to perform a prediction task.

```{r helper functions}

# Function to append shift values (lags or leads) to data frame
append_shifts = function(df, shifts) {
  # Make sure that we have a complete record of dates for each geo_value (fill
  # with NAs as necessary)
  df_all = df %>% group_by(geo_value) %>%
    summarize(time_value = seq.Date(as.Date(min(time_value)),
                                    as.Date(max(time_value)),
                                    by = "day")) %>% ungroup()
  df = full_join(df, df_all, by = c("geo_value", "time_value"))
  
  # Group by geo value, sort rows by increasing time
  df = df %>% group_by(geo_value) %>% arrange(time_value) 
  
  # Load over shifts, and add lag value or lead value
  for (shift in shifts) {
    fun = ifelse(shift < 0, lag, lead)
    varname = sprintf("value%+d", shift)
    df = mutate(df, !!varname := fun(value, n = abs(shift)))
  }
  
  # Ungroup and return
  return(ungroup(df))
}

```


```{r prediction_setup, warning=FALSE, message=FALSE}

# Filter to complete states, transform the signals, append 1-2 week lags to 
# all three, and also 1-2 week leads to case rates
# Rescale all the signals to bring them down to proportions (between 0 and 1)
prefixes = c("dv", "fb_cli", "fb_cli_com", "hm", "anosmia", "ageusia", "aa")
rescale_case = 1e5 # Originally a count per 100,000 people

df_cases_trans = df_cases %>%
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_case)) %>% 
  append_shifts(shifts = c(lags, leads))
colnames( df_cases_trans ) = sub("^value", "case", colnames( df_cases_trans ))

combined_df = df_cases_trans

df_signals_trans = vector("list", length(signals))
for (i in 1:length(signals)) {
  if (i >= 4) {
    rescale_factor = max(df_signals[[i]]$value)
  } else {
    rescale_factor = 1e2
  }
    
  df_signals_trans[[i]] = df_signals[[i]] %>% 
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_factor)) %>% 
  append_shifts(shifts = lags) 

  # Rename columns
  colnames( df_signals_trans[[i]] ) = sub("^value", prefixes[i], colnames( df_signals_trans[[i]] ))
  
  # Make one big matrix by joining these three data frames
  combined_df = full_join(combined_df, df_signals_trans[[i]], by = c("geo_value", "time_value"))
}
```


```{r, warning=FALSE, message=FALSE, cache=TRUE}
##### Analysis #####

model_names = c("Cases", "Cases + DV", "Cases + FB-CLI",  "Cases + FB-CLI-community",
                "Cases + Hospitalization", "Cases + GS-Anosmia", "Cases + GS-Ageusia",
                "Cases + Anosmia+Ageusia")

res_list = vector("list", length = length(leads)) 
n = 14 # Number of trailing days to use for training set
verbose = FALSE # Print intermediate progress to console?
lp_solver = "gurobi" # LP solver to use in quantile_lasso(); "gurobi" or "glpk"

# Loop over lead, forecast dates, build models, and record errors (warning: this
# computation takes a while)
for (i in 1:length(leads)) { 
  lead = leads[i]; if (verbose) cat("***", lead, "***\n")
  
  # Create a data frame to store our results. Code below populates its rows in a
  # way that breaks from typical dplyr operations, done for efficiency
  res_list[[i]] = combined_df %>% 
    filter(between(time_value, as.Date(start_day) - min(lags) + lead, 
                   as.Date(end_day) - lead)) %>%
    select(geo_value, time_value) %>%
    mutate(err0 = as.double(NA), err1 = as.double(NA), err2 = as.double(NA), 
           err3 = as.double(NA), err4 = as.double(NA), err5 = as.double(NA),
           err6 = as.double(NA), err7 = as.double(NA), err8 = as.double(NA),
           lead = lead) 
  valid_dates = unique(res_list[[i]]$time_value)
  
  for (j in 1:length(valid_dates)) {
    date = valid_dates[j]; if (verbose) cat(format(date), "... ")
    
    # Filter down to training set and test set
    z_tr = combined_df %>% filter(between(time_value, date - lead - n, date - lead))
    z_te = combined_df %>% filter(time_value == date)
    inds = which(res_list[[i]]$time_value == date)
    
    # Create training and test responses
    y_tr = z_tr %>% pull(paste0("case+", lead))
    y_te = z_te %>% pull(paste0("case+", lead))
    
    # Strawman model
    if (verbose) cat("0")
    y_hat = z_te %>% pull(case)
    res_list[[i]][inds,]$err0 = abs(inv_trans(y_hat) - inv_trans(y_te))
    
    # Cases only model
    if (verbose) cat("1")
    x_tr_case = z_tr %>% select(starts_with("case") & !contains("+"))
    x_te_case = z_te %>% select(starts_with("case") & !contains("+"))
    x_tr = x_tr_case; x_te = x_te_case # For symmetry wrt what follows 
    ok = complete.cases(x_tr, y_tr)
    if (sum(ok) > 0) {
      obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                           lambda = 0, stand = FALSE, lp_solver = lp_solver)
      y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
      res_list[[i]][inds,]$err1 = abs(inv_trans(y_hat) - inv_trans(y_te)) 
    }
    
    # Cases and another signal model
    for (k in 1:length(signals)){
      if (verbose) cat(as.character(k+1))
      x_tr_ght = z_tr %>% select(starts_with(prefixes[k]))
      x_te_ght = z_te %>% select(starts_with(prefixes[k]))
      x_tr = cbind(x_tr_case, x_tr_ght)
      x_te = cbind(x_te_case, x_te_ght)
      ok = complete.cases(x_tr, y_tr)
      if (sum(ok) > 0) {
        obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                             lambda = 0, stand = FALSE, lp_solver = lp_solver)
        y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
        err_vec = abs(inv_trans(y_hat) - inv_trans(y_te))
        err_col = paste('err', as.character(k+1), sep="")
        res_list[[i]][inds,][err_col] = err_vec
      }
    }
  }
}

```



```{r, warning=FALSE, message=FALSE}
# Bind results over different leads into one big data frame
res = do.call(rbind, res_list)

# Calculate the median of the scaled errors for the various model: that is, the 
# errors relative to the strawman's error
res_med = res %>% mutate(err1 = err1 / err0, err2 = err2 / err0,
                         err3 = err3 / err0, err4 = err4 / err0,
                         err5 = err5 / err0, err6 = err6 / err0,
                         err7 = err7 / err0, err8 = err8 / err0) %>%
  select(-err0) %>% 
  tidyr::pivot_longer(names_to = "model", values_to = "err", 
                      cols = -c(geo_value, time_value, lead)) %>%
  group_by(time_value, lead, model) %>% 
  summarize(err = median(err, na.rm = TRUE)) %>%
  ungroup() %>% 
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = model_names))

ggplot(res_med, aes(x = time_value, y = err)) + 
  geom_line(aes(color = model)) + 
  geom_hline(yintercept = 1, linetype = 2, color = "gray") +
  facet_wrap(vars(lead)) + 
  labs(x = "Date", y = "Scaled error", title = "Id transform") +
  theme_bw() + theme(legend.pos = "bottom", legend.title = element_blank())
```

We can see that during some time periods, median scaled error for the cases + google-symptoms (anosmia, ageusia, A+A) model is more variable than other signals at county level which is probably due to low geographical coverage (only ~100 counties available).

```{r compare_media_err_over_all_time_and_states}
# Restrict to common period for all models, then calculate the scaled errors 
# for each model, that is, the error relative to the strawman's error
res_all = res %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0,       # Compute relative error
         err3 = err3 / err0, err4 = err4 / err0,
         err5 = err5 / err0, err6 = err6 / err0,
         err7 = err7 / err0, err8 = err8 / err0# to strawman model
         ) %>%
  mutate(dif12 = err1 - err2, dif13 = err1 - err3, 
         dif14 = err1 - err4, dif14 = err1 - err4,
         dif15 = err1 - err5, dif16 = err1 - err6, 
         dif17 = err1 - err7, dif18 = err1 - err8# Compute differences
   # relative to cases model
         ) %>%
  ungroup() %>%
  select(-err0) 
         
# Calculate and print median errors, for all models
res_err = res_all %>% 
  select(-starts_with("dif")) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = model_names))

knitr::kable(res_err %>%
               group_by(model, lead) %>%
               summarize(err = round(median(err), 3), n = length(unique(time_value))) %>% 
               arrange(lead) %>% ungroup() %>%
               rename("Model" = model, "Median scaled error" = err, 
                      "Target" = lead, "Test days" = n), 
             caption = paste("Test period:", min(res_err$time_value), "to",
                             max(res_err$time_value), ", common time period"),
             format = "html", table.attr = "style='width:70%;'")
```


Calculating median scaled error over all dates and all counties for a given model, we see that adding symptoms as a predictor reduces median scaled error by approximately 10 percentage points (and approximately 10%) compared to a cases-only model.

"Are these differences in median scaled errors significant? It’s hard to say [for certain, given the spatial and temporal dependence of the data], but some basic hypothesis testing suggests that they probably are: below we conduct a sign test for whether the difference in the “Cases” model’s scaled error and each other model’s scaled error is centered at zero" ([source](https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/)).

```{r is difference in scaled error between models centered at 0?}
comparison_names = paste("Cases vs", model_names[2:8])

# Compute p-values using the sign test against a one-sided alternative, for
# all models
res_dif = res_all %>%
  select(-starts_with("err")) %>%
  pivot_longer(names_to = "model", values_to = "dif",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, 
                        labels = comparison_names)) 

knitr::kable(res_dif %>%
               group_by(model, lead) %>%
               summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                        n = n(), alt = "greater")$p.val) %>%
               ungroup() %>%
               rename("Comparison" = model, "Target" = lead, "P-value" = p), 
             format = "html", table.attr = "style='width:50%;'")
```

Differences in model fit are (very) significantly different, ignoring dependence structure at the moment. 

"To mitigate the dependence across time (which intuitively seems to matter more than that across space), we recomputed these tests in a stratified way, where for each day we run a sign test on the scaled errors between two models over all [geographic regions]. The results are plotted as histograms below", where one count corresponds to one prediction date ([source](https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/)).


```{r calc p-values from sign test across all states for each date}
# Red, blue (similar to ggplot defaults), then yellow.
ggplot_colors = c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
                  "#FF7F00", "#FFFF33", "#A65628")

ggplot(res_dif %>% 
         group_by(model, lead, time_value) %>%
         summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                  n = n(), alt = "greater")$p.val) %>%
         ungroup(), aes(p)) +
  geom_histogram(aes(color = model, fill = model), alpha = 0.4) + 
  scale_color_manual(values = ggplot_colors) +
  scale_fill_manual(values = ggplot_colors) +
  facet_wrap(vars(lead, model)) + 
  labs(x = "P-value", y = "Count") +
  theme_bw() + theme(legend.pos = "none")
```

If the differences between models were *not* significant, we'd expect the distribution of p-values to be uniform. That is distinctly not what we see here. Model differences at both 7 and 14 days ahead have elevated (compared to expected) numbers of low p-values, suggesting significance. However, 14-day ahead comparisons look *more* significant across all alternative models. So it appears that additional predictors, especially GS, are useful for farther-future predictions, rather than near-future where number of cases is largely sufficient.


```{r prediction_setup over days ahead, include=FALSE, cache=TRUE, warning=FLASE, message=FALSE}
# Set how far to forecast ahead.
leads = 5:20
lags = 1:2 * -7
rm(lead)

prefixes = c("dv", "fb_cli", "fb_cli_com", "hm", "anosmia", "ageusia", "aa")
rescale_case = 1e5 # Originally a count per 100,000 people

df_cases_trans = df_cases %>%
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_case)) %>% 
  append_shifts(shifts = c(lags, leads))
colnames( df_cases_trans ) = sub("^value", "case", colnames( df_cases_trans ))

combined_df = df_cases_trans

df_signals_trans = vector("list", length(signals))
for (i in 1:length(signals)) {
  if (i >= 4) {
    rescale_factor = max(df_signals[[i]]$value)
  } else {
    rescale_factor = 1e2
  }
    
  df_signals_trans[[i]] = df_signals[[i]] %>% 
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_factor)) %>% 
  append_shifts(shifts = lags) 

  # Rename columns
  colnames( df_signals_trans[[i]] ) = sub("^value", prefixes[i], colnames( df_signals_trans[[i]] ))
  
  # Make one big matrix by joining these three data frames
  combined_df = full_join(combined_df, df_signals_trans[[i]], by = c("geo_value", "time_value"))
}
```

```{r, warning=FALSE, message=FALSE}
##### Analysis #####
model_names = c("Cases", "Cases + DV", "Cases + FB-CLI",  "Cases + FB-CLI-community",
                "Cases + Hospitalization", "Cases + GS-Anosmia", "Cases + GS-Ageusia",
                "Cases + Anosmia+Ageusia")

res_list = vector("list", length = length(leads)) 
n = 14 # Number of trailing days to use for training set
verbose = FALSE # Print intermediate progress to console?
lp_solver = "gurobi" # LP solver to use in quantile_lasso(); "gurobi" or "glpk"

# Loop over lead, forecast dates, build models, and record errors (warning: this
# computation takes a while)
for (i in 1:length(leads)) { 
  lead = leads[i]; if (verbose) cat("***", lead, "***\n")
  
  # Create a data frame to store our results. Code below populates its rows in a
  # way that breaks from typical dplyr operations, done for efficiency
  res_list[[i]] = combined_df %>% 
    filter(between(time_value, as.Date(start_day) - min(lags) + lead, 
                   as.Date(end_day) - lead)) %>%
    select(geo_value, time_value) %>%
    mutate(err0 = as.double(NA), err1 = as.double(NA), err2 = as.double(NA), 
           err3 = as.double(NA), err4 = as.double(NA), err5 = as.double(NA),
           err6 = as.double(NA), err7 = as.double(NA), err8 = as.double(NA),
           lead = lead) 
  valid_dates = unique(res_list[[i]]$time_value)
  
  for (j in 1:length(valid_dates)) {
    date = valid_dates[j]; if (verbose) cat(format(date), "... ")
    
    # Filter down to training set and test set
    z_tr = combined_df %>% filter(between(time_value, date - lead - n, date - lead))
    z_te = combined_df %>% filter(time_value == date)
    inds = which(res_list[[i]]$time_value == date)
    
    # Create training and test responses
    y_tr = z_tr %>% pull(paste0("case+", lead))
    y_te = z_te %>% pull(paste0("case+", lead))
    
    # Strawman model
    if (verbose) cat("0")
    y_hat = z_te %>% pull(case)
    res_list[[i]][inds,]$err0 = abs(inv_trans(y_hat) - inv_trans(y_te))
    
    # Cases only model
    if (verbose) cat("1")
    x_tr_case = z_tr %>% select(starts_with("case") & !contains("+"))
    x_te_case = z_te %>% select(starts_with("case") & !contains("+"))
    x_tr = x_tr_case; x_te = x_te_case # For symmetry wrt what follows 
    ok = complete.cases(x_tr, y_tr)
    if (sum(ok) > 0) {
      obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                           lambda = 0, stand = FALSE, lp_solver = lp_solver)
      y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
      res_list[[i]][inds,]$err1 = abs(inv_trans(y_hat) - inv_trans(y_te)) 
    }
    
    # Cases and another signal model
    for (k in 1:length(signals)){
      if (verbose) cat(as.character(k+1))
      x_tr_ght = z_tr %>% select(starts_with(prefixes[k]))
      x_te_ght = z_te %>% select(starts_with(prefixes[k]))
      x_tr = cbind(x_tr_case, x_tr_ght)
      x_te = cbind(x_te_case, x_te_ght)
      ok = complete.cases(x_tr, y_tr)
      if (sum(ok) > 0) {
        obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                             lambda = 0, stand = FALSE, lp_solver = lp_solver)
        y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
        err_vec = abs(inv_trans(y_hat) - inv_trans(y_te))
        err_col = paste('err', as.character(k+1), sep="")
        res_list[[i]][inds,][err_col] = err_vec
      }
    }
  }
}

# Bind results over different leads into one big data frame
res = do.call(rbind, res_list)

```



```{r median error between all models by num days forecast ahead}
# Compute and plot median errors as function of number of days ahead.
err_by_lead = res %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0, err3 = err3 / err0, err4 = err4 / err0,
         err5 = err5 / err0, err6 = err6 / err0, err7 = err7 / err0, err8 = err8 / err0) %>%  # Compute relative error to strawman model
  ungroup() %>%
  select(-err0) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(model = factor(model, labels = model_names[1:8])) %>%
  group_by(model, lead) %>%
  summarize(err = median(err)) %>% 
  ungroup()

ggplot(err_by_lead, aes(x = lead, y = err)) + 
  geom_line(aes(color = model)) + 
  geom_point(aes(color = model)) + 
  scale_color_manual(values = c("black", ggplot_colors)) +
  geom_hline(yintercept = err_by_lead %>% 
               filter(lead %in% 7, model == "Cases") %>% pull(err),
             linetype = 2, color = "gray") +
  labs(title = "Forecasting errors by number of days ahead",
       subtitle = sprintf("Over all states"),
       x = "Number of days ahead", y = "Median scaled error") +
  theme_bw() + theme(legend.pos = "bottom", legend.title = element_blank()) +
  guides(color = guide_legend(nrow = 3))
```


We can see that models containing GS outperform other models including Case-only and Cases + other signals. Errors are calculated on a shared-time basis. With all models together, more dates are dropped and it causes shifts in some error values. In fact, it shows that the prediction of cases + GS model is of high quality though with low geographical coverage. 



## MSA Level Analysis
### Getting data
```{r, echo = FALSE, message = FALSE, warning=FALSE, cache=TRUE}
# Fetch the following sources and signals from the API 
sources = c("doctor-visits", "fb-survey", "fb-survey", "ght",
            "hospital-admissions")
signals = c("smoothed_adj_cli", "smoothed_cli", "smoothed_hh_cmnty_cli", 
            "smoothed_search", "smoothed_adj_covid19")
names = c("Doctor visits", "Facebook CLI", "Facebook CLI-in-community", 
          "Google trends", "Hospitalizations")

start_day = "2020-04-15"
end_day = "2020-10-15"

# Fetch USAFacts confirmed case incidence proportion (smoothed with 7-day 
# trailing average)
df_cases = covidcast_signal("usa-facts", "confirmed_7dav_incidence_prop",
                            start_day, end_day, "msa")

# Get geo values that appear in all datasets.
geo_values_complete = df_cases$geo_value
df_signals = vector("list", length(signals))
for (i in 1:length(signals)) {
  df_signals[[i]] = covidcast_signal(sources[i], signals[i], start_day, end_day, "msa")
  geo_values_complete = intersect(geo_values_complete, df_signals[[i]]$geo_value)
}

n = length(signals) + 3

library(stringr)
dir = "../test_files/google-symptoms/"
df_signals[[n-2]] = read.csv(paste(dir,"msa_ageusia_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date"))  %>%
                                    filter(time_value >= as.Date(start_day) 
                                           &  time_value <= as.Date(end_day))
df_signals[[n-2]]$geo_value = str_pad(df_signals[[n-2]]$geo_value, width=5, side="left", pad="0")
sources[n-2] = "google-symptoms"
signals[n-2] = "ageusia_smoothed_search"
names[n-2] = "GS Ageusia Smoothed Search"

df_signals[[n-1]] = read.csv(paste(dir,"msa_anosmia_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date")) %>%
                                    filter(time_value >= as.Date(start_day)
                                           &  time_value <= as.Date(end_day))
df_signals[[n-1]]$geo_value = str_pad(df_signals[[n-1]]$geo_value, width=5, side="left", pad="0")
sources[n-1] = "google-symptoms"
signals[n-1] = "anosmia_smoothed_search"
names[n-1] = "GS Anosmia Smoothed Search"

df_signals[[n]] = read.csv(paste(dir,"msa_combined_symptoms_smoothed_search.csv", sep = ""),
                                             colClasses=c("geo_value"="character", "time_value"="Date"))  %>%
                                    filter(time_value >= as.Date(start_day)
                                           &  time_value <= as.Date(end_day))
df_signals[[n]]$geo_value = str_pad(df_signals[[n]]$geo_value, width=5, side="left", pad="0")
sources[n] = "google-symptoms"
signals[n] = "combined_symptoms_smoothed_search"
names[n] = "GS A+A Smoothed Search"
```


```{r, warning=FALSE}
# Consider only counties with at least 200 cumulative cases
case_num = 200
geo_values = covidcast_signal("usa-facts", "confirmed_cumulative_num",
                              max(df_cases$time_value), 
                              max(df_cases$time_value), 
                              geo_type = "msa") %>%
  filter(value >= case_num) %>% pull(geo_value)

geo_values = intersect(geo_values, geo_values_complete)
```


### Prediction Preparation
Here we use code liberally borrowed from a
[blog post](https://github.com/cmu-delphi/delphi-blog/blob/main/content/post/2020-09-21-forecast-demo.Rmd)
to perform a prediction task.

```{r prediction_setup, warning=FALSE, message=FALSE}
rm(lead)

# Filter to complete states, transform the signals, append 1-2 week lags to 
# all three, and also 1-2 week leads to case rates
# Rescale all the signals to bring them down to proportions (between 0 and 1)
prefixes = c("dv", "fb_cli", "fb_cli_com", "ght", "hm", "anosmia", "ageusia", "aa")
rescale_case = 1e5 # Originally a count per 100,000 people

df_cases_trans = df_cases %>%
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_case)) %>% 
  append_shifts(shifts = c(lags, leads))
colnames( df_cases_trans ) = sub("^value", "case", colnames( df_cases_trans ))

combined_df = df_cases_trans

df_signals_trans = vector("list", length(signals))
for (i in 1:length(signals)) {
  if (i >= 4) {
    rescale_factor = max(df_signals[[i]]$value)
  } else {
    rescale_factor = 1e2
  }
    
  df_signals_trans[[i]] = df_signals[[i]] %>% 
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_factor)) %>% 
  append_shifts(shifts = lags) 

  # Rename columns
  colnames( df_signals_trans[[i]] ) = sub("^value", prefixes[i], colnames( df_signals_trans[[i]] ))
  
  # Make one big matrix by joining these three data frames
  combined_df = full_join(combined_df, df_signals_trans[[i]], by = c("geo_value", "time_value"))
}
```


```{r, warning=FALSE, message=FALSE, cache=TRUE}
##### Analysis #####

model_names = c("Cases", "Cases + DV", "Cases + FB-CLI",  "Cases + FB-CLI-community",
                "Cases + GHT", "Cases + Hospitalization", "Cases + GS-Anosmia",
                "Cases + GS-Ageusia", "Cases + Anosmia+Ageusia")

res_list = vector("list", length = length(leads)) 
n = 14 # Number of trailing days to use for training set
verbose = FALSE # Print intermediate progress to console?
lp_solver = "gurobi" # LP solver to use in quantile_lasso(); "gurobi" or "glpk"

# Loop over lead, forecast dates, build models, and record errors (warning: this
# computation takes a while)
for (i in 1:length(leads)) { 
  lead = leads[i]; if (verbose) cat("***", lead, "***\n")
  
  # Create a data frame to store our results. Code below populates its rows in a
  # way that breaks from typical dplyr operations, done for efficiency
  res_list[[i]] = combined_df %>% 
    filter(between(time_value, as.Date(start_day) - min(lags) + lead, 
                   as.Date(end_day) - lead)) %>%
    select(geo_value, time_value) %>%
    mutate(err0 = as.double(NA), err1 = as.double(NA), err2 = as.double(NA), 
           err3 = as.double(NA), err4 = as.double(NA), err5 = as.double(NA),
           err6 = as.double(NA), err7 = as.double(NA), err8 = as.double(NA),
           err9 = as.double(NA), lead = lead) 
  valid_dates = unique(res_list[[i]]$time_value)
  
  for (j in 1:length(valid_dates)) {
    date = valid_dates[j]; if (verbose) cat(format(date), "... ")
    
    # Filter down to training set and test set
    z_tr = combined_df %>% filter(between(time_value, date - lead - n, date - lead))
    z_te = combined_df %>% filter(time_value == date)
    inds = which(res_list[[i]]$time_value == date)
    
    # Create training and test responses
    y_tr = z_tr %>% pull(paste0("case+", lead))
    y_te = z_te %>% pull(paste0("case+", lead))
    
    # Strawman model
    if (verbose) cat("0")
    y_hat = z_te %>% pull(case)
    res_list[[i]][inds,]$err0 = abs(inv_trans(y_hat) - inv_trans(y_te))
    
    # Cases only model
    if (verbose) cat("1")
    x_tr_case = z_tr %>% select(starts_with("case") & !contains("+"))
    x_te_case = z_te %>% select(starts_with("case") & !contains("+"))
    x_tr = x_tr_case; x_te = x_te_case # For symmetry wrt what follows 
    ok = complete.cases(x_tr, y_tr)
    if (sum(ok) > 0) {
      obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                           lambda = 0, stand = FALSE, lp_solver = lp_solver)
      y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
      res_list[[i]][inds,]$err1 = abs(inv_trans(y_hat) - inv_trans(y_te)) 
    }
    
    # Cases and another signal model
    for (k in 1:length(signals)){
      if (verbose) cat(as.character(k+1))
      x_tr_signal = z_tr %>% select(starts_with(prefixes[k]))
      x_te_signal = z_te %>% select(starts_with(prefixes[k]))
      x_tr = cbind(x_tr_case, x_tr_signal)
      x_te = cbind(x_te_case, x_te_signal)
      ok = complete.cases(x_tr, y_tr)
      if (sum(ok) > 0) {
        obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                             lambda = 0, stand = FALSE, lp_solver = lp_solver)
        y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
        err_vec = abs(inv_trans(y_hat) - inv_trans(y_te))
        err_col = paste('err', as.character(k+1), sep="")
        res_list[[i]][inds,][err_col] = err_vec
      }
    }
  }
}

```



```{r, warning=FALSE, message=FALSE}
# Bind results over different leads into one big data frame
res = do.call(rbind, res_list)

# Calculate the median of the scaled errors for the various model: that is, the 
# errors relative to the strawman's error
res_med = res %>% mutate(err1 = err1 / err0, err2 = err2 / err0,
                         err3 = err3 / err0, err4 = err4 / err0,
                         err5 = err5 / err0, err6 = err6 / err0,
                         err7 = err7 / err0, err8 = err8 / err0,
                         err9 = err9 / err0) %>%
  select(-err0) %>% 
  tidyr::pivot_longer(names_to = "model", values_to = "err", 
                      cols = -c(geo_value, time_value, lead)) %>%
  group_by(time_value, lead, model) %>% 
  summarize(err = median(err, na.rm = TRUE)) %>%
  ungroup() %>% 
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = model_names))

ggplot(res_med, aes(x = time_value, y = err)) + 
  geom_line(aes(color = model)) + 
  geom_hline(yintercept = 1, linetype = 2, color = "gray") +
  facet_wrap(vars(lead)) + 
  labs(x = "Date", y = "Scaled error", title = "Id transform") +
  theme_bw() + theme(legend.pos = "bottom", legend.title = element_blank())
```


```{r compare_media_err_over_all_time_and_states}
# Restrict to common period for all models, then calculate the scaled errors 
# for each model, that is, the error relative to the strawman's error
res_all = res %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0,       # Compute relative error
         err3 = err3 / err0, err4 = err4 / err0,
         err5 = err5 / err0, err6 = err6 / err0,
         err7 = err7 / err0, err8 = err8 / err0,
         err9 = err9 / err0# to strawman model
         ) %>%
  mutate(dif12 = err1 - err2, dif13 = err1 - err3, 
         dif14 = err1 - err4, dif14 = err1 - err4,
         dif15 = err1 - err5, dif16 = err1 - err6, 
         dif17 = err1 - err7, dif18 = err1 - err8,
         dif19 = err1 - err9# Compute differences
   # relative to cases model
         ) %>%
  ungroup() %>%
  select(-err0) 
         
# Calculate and print median errors, for all models
res_err = res_all %>% 
  select(-starts_with("dif")) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = model_names))

knitr::kable(res_err %>%
               group_by(model, lead) %>%
               summarize(err = round(median(err), 3), n = length(unique(time_value))) %>% 
               arrange(lead) %>% ungroup() %>%
               rename("Model" = model, "Median scaled error" = err, 
                      "Target" = lead, "Test days" = n), 
             caption = paste("Test period:", min(res_err$time_value), "to",
                             max(res_err$time_value), ", common time period"),
             format = "html", table.attr = "style='width:70%;'")
```


"Are these differences in median scaled errors significant? It’s hard to say [for certain, given the spatial and temporal dependence of the data], but some basic hypothesis testing suggests that they probably are: below we conduct a sign test for whether the difference in the “Cases” model’s scaled error and each other model’s scaled error is centered at zero" ([source](https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/)).

```{r is difference in scaled error between models centered at 0?}
comparison_names = paste("Cases vs", model_names[2:9])

# Compute p-values using the sign test against a one-sided alternative, for
# all models
res_dif = res_all %>%
  select(-starts_with("err")) %>%
  pivot_longer(names_to = "model", values_to = "dif",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, 
                        labels = comparison_names)) 

knitr::kable(res_dif %>%
               group_by(model, lead) %>%
               summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                        n = n(), alt = "greater")$p.val) %>%
               ungroup() %>%
               rename("Comparison" = model, "Target" = lead, "P-value" = p), 
             format = "html", table.attr = "style='width:50%;'")
```

Differences in model fit are (very) significantly different, ignoring dependence structure at the moment. 

"To mitigate the dependence across time (which intuitively seems to matter more than that across space), we recomputed these tests in a stratified way, where for each day we run a sign test on the scaled errors between two models over all [geographic regions]. The results are plotted as histograms below", where one count corresponds to one prediction date ([source](https://delphi.cmu.edu/blog/2020/09/21/can-symptoms-surveys-improve-covid-19-forecasts/)).


```{r calc p-values from sign test across all states for each date}
# Red, blue (similar to ggplot defaults), then yellow.
ggplot_colors = c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
                  "#FF7F00", "#FFFF33", "#A65628", "#F781BF")

ggplot(res_dif %>% 
         group_by(model, lead, time_value) %>%
         summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                  n = n(), alt = "greater")$p.val) %>%
         ungroup(), aes(p)) +
  geom_histogram(aes(color = model, fill = model), alpha = 0.4) + 
  scale_color_manual(values = ggplot_colors) +
  scale_fill_manual(values = ggplot_colors) +
  facet_wrap(vars(lead, model)) + 
  labs(x = "P-value", y = "Count") +
  theme_bw() + theme(legend.pos = "none")
```

If the differences between models were *not* significant, we'd expect the distribution of p-values to be uniform. That is distinctly not what we see here. Model differences at both 7 and 14 days ahead have elevated (compared to expected) numbers of low p-values, suggesting significance. However, 14-day ahead comparisons look *more* significant across all alternative models. So it appears that additional predictors, especially GS, are useful for farther-future predictions, rather than near-future where number of cases is largely sufficient.


```{r prediction_setup over days ahead, include=FALSE, cache=TRUE, warning=FLASE, message=FALSE}
# Set how far to forecast ahead.
leads = 5:20
lags = 1:2 * -7
rm(lead)

prefixes = c("dv", "fb_cli", "fb_cli_com", "ght", "hm", "anosmia", "ageusia", "aa")
rescale_case = 1e5 # Originally a count per 100,000 people

df_cases_trans = df_cases %>%
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_case)) %>% 
  append_shifts(shifts = c(lags, leads))
colnames( df_cases_trans ) = sub("^value", "case", colnames( df_cases_trans ))

combined_df = df_cases_trans

df_signals_trans = vector("list", length(signals))
for (i in 1:length(signals)) {
  if (i >= 4) {
    rescale_factor = max(df_signals[[i]]$value)
  } else {
    rescale_factor = 1e2
  }
    
  df_signals_trans[[i]] = df_signals[[i]] %>% 
  select(geo_value, time_value, value) %>%
  filter(geo_value %in% geo_values_complete) %>% 
  mutate(value = trans(value / rescale_factor)) %>% 
  append_shifts(shifts = lags) 

  # Rename columns
  colnames( df_signals_trans[[i]] ) = sub("^value", prefixes[i], colnames( df_signals_trans[[i]] ))
  
  # Make one big matrix by joining these three data frames
  combined_df = full_join(combined_df, df_signals_trans[[i]], by = c("geo_value", "time_value"))
}
```

```{r, warning=FALSE, message=FALSE}
##### Analysis #####
model_names = c("Cases", "Cases + DV", "Cases + FB-CLI",  "Cases + FB-CLI-community",
                "Cases + Hospitalization", "Cases + GHT", "Cases + GS-Anosmia",
                "Cases + GS-Ageusia",
                "Cases + Anosmia+Ageusia")

res_list = vector("list", length = length(leads)) 
n = 14 # Number of trailing days to use for training set
verbose = FALSE # Print intermediate progress to console?
lp_solver = "gurobi" # LP solver to use in quantile_lasso(); "gurobi" or "glpk"

# Loop over lead, forecast dates, build models, and record errors (warning: this
# computation takes a while)
for (i in 1:length(leads)) { 
  lead = leads[i]; if (verbose) cat("***", lead, "***\n")
  
  # Create a data frame to store our results. Code below populates its rows in a
  # way that breaks from typical dplyr operations, done for efficiency
  res_list[[i]] = combined_df %>% 
    filter(between(time_value, as.Date(start_day) - min(lags) + lead, 
                   as.Date(end_day) - lead)) %>%
    select(geo_value, time_value) %>%
    mutate(err0 = as.double(NA), err1 = as.double(NA), err2 = as.double(NA), 
           err3 = as.double(NA), err4 = as.double(NA), err5 = as.double(NA),
           err6 = as.double(NA), err7 = as.double(NA), err8 = as.double(NA),
           err9 = as.double(NA), lead = lead) 
  valid_dates = unique(res_list[[i]]$time_value)
  
  for (j in 1:length(valid_dates)) {
    date = valid_dates[j]; if (verbose) cat(format(date), "... ")
    
    # Filter down to training set and test set
    z_tr = combined_df %>% filter(between(time_value, date - lead - n, date - lead))
    z_te = combined_df %>% filter(time_value == date)
    inds = which(res_list[[i]]$time_value == date)
    
    # Create training and test responses
    y_tr = z_tr %>% pull(paste0("case+", lead))
    y_te = z_te %>% pull(paste0("case+", lead))
    
    # Strawman model
    if (verbose) cat("0")
    y_hat = z_te %>% pull(case)
    res_list[[i]][inds,]$err0 = abs(inv_trans(y_hat) - inv_trans(y_te))
    
    # Cases only model
    if (verbose) cat("1")
    x_tr_case = z_tr %>% select(starts_with("case") & !contains("+"))
    x_te_case = z_te %>% select(starts_with("case") & !contains("+"))
    x_tr = x_tr_case; x_te = x_te_case # For symmetry wrt what follows 
    ok = complete.cases(x_tr, y_tr)
    if (sum(ok) > 0) {
      obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                           lambda = 0, stand = FALSE, lp_solver = lp_solver)
      y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
      res_list[[i]][inds,]$err1 = abs(inv_trans(y_hat) - inv_trans(y_te)) 
    }
    
    # Cases and another signal model
    for (k in 1:length(signals)){
      if (verbose) cat(as.character(k+1))
      x_tr_ght = z_tr %>% select(starts_with(prefixes[k]))
      x_te_ght = z_te %>% select(starts_with(prefixes[k]))
      x_tr = cbind(x_tr_case, x_tr_ght)
      x_te = cbind(x_te_case, x_te_ght)
      ok = complete.cases(x_tr, y_tr)
      if (sum(ok) > 0) {
        obj = quantile_lasso(as.matrix(x_tr[ok,]), y_tr[ok], tau = 0.5,
                             lambda = 0, stand = FALSE, lp_solver = lp_solver)
        y_hat = as.numeric(predict(obj, newx = as.matrix(x_te)))
        err_vec = abs(inv_trans(y_hat) - inv_trans(y_te))
        err_col = paste('err', as.character(k+1), sep="")
        res_list[[i]][inds,][err_col] = err_vec
      }
    }
  }
}

# Bind results over different leads into one big data frame
res = do.call(rbind, res_list)

```



```{r median error between all models by num days forecast ahead}
# Compute and plot median errors as function of number of days ahead.
err_by_lead = res %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0, err3 = err3 / err0, err4 = err4 / err0,
         err5 = err5 / err0, err6 = err6 / err0, err7 = err7 / err0, err8 = err8 / err0,
         err9 = err9 / err0) %>%  # Compute relative error to strawman model
  ungroup() %>%
  select(-err0) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(model = factor(model, labels = model_names[1:9])) %>%
  group_by(model, lead) %>%
  summarize(err = median(err)) %>% 
  ungroup()

ggplot(err_by_lead, aes(x = lead, y = err)) + 
  geom_line(aes(color = model)) + 
  geom_point(aes(color = model)) + 
  scale_color_manual(values = c("black", ggplot_colors)) +
  geom_hline(yintercept = err_by_lead %>% 
               filter(lead %in% 7, model == "Cases") %>% pull(err),
             linetype = 2, color = "gray") +
  labs(title = "Forecasting errors by number of days ahead",
       subtitle = sprintf("Over all states"),
       x = "Number of days ahead", y = "Median scaled error") +
  theme_bw() + theme(legend.pos = "bottom", legend.title = element_blank()) +
  guides(color = guide_legend(nrow = 4))
```


