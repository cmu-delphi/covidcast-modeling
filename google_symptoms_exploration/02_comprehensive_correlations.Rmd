---
title: "Google Symptoms Dataset"
author: "Addison"
output:
  html_document:
    toc: true
    code_folding: hide
---

```{r import_statements, echo = FALSE, message = FALSE}
library(tidyverse)
library(covidcast)
library(dplyr)
library(ggplot2)
```

```{r declare_global_constants}
POSTAL_TO_STATE = list('AL'='Alabama', 'AK'='Alaska', 'AS'='American Samoa',
                       'AZ'='Arizona', 'AR'='Arkansas', 'CA'='California',
                       'CO'='Colorado', 'CT'='Connecticut', 'DE'='Delaware',
                       'DC'='District of Columbia', 'FL'='Florida',
                       'GA'='Georgia', 'GU'='Guam', 'HI'='Hawaii',
                       'ID'='Idaho', 'IL'='Illinois', 'IN'='Indiana',
                       'IA'='Iowa', 'KS'='Kansas', 'KY'='Kentucky',
                       'LA'='Louisiana', 'ME'='Maine', 'MD'='Maryland',
                       'MA'='Massachusetts', 'MI'='Michigan', 'MN'='Minnesota',
                       'MS'='Mississippi', 'MO'='Missouri', 'MT'='Montana',
                       'NE'='Nebraska', 'NV'='Nevada', 'NH'='New Hampshire',
                       'NJ'='New Jersey', 'NM'='New Mexico', 'NY'='New York',
                       'NC'='North Carolina', 'ND'='North Dakota',
                       'MP'='Northern Mariana Islands', 'OH'='Ohio',
                       'OK'='Oklahoma', 'OR'='Oregon', 'PA'='Pennsylvania',
                       'PR'='Puerto Rico', 'RI'='Rhode Island', 'SC'='South Carolina',
                       'SD'='South Dakota', 'TN'='Tennessee',
                       'TX'='Texas', 'UT'='Utah', 'VT'='Vermont', 'VI'='Virgin Islands',
                       'VA'='Virginia', 'WA'='Washington', 'WV'='West Virginia',
                       'WI'='Wisconsin', 'WY'='Wyoming')

states = c("al", "ak", "az", "ar", "ca", "co", "ct", "de", "fl", "ga", "hi",
           "id", "il", "in", "ia", "ks", "ky", "la", "me", "md", "ma", "mi",
           "mn", "ms", "mo", "mt", "ne", "nv", "nh", "nj", "nm", "ny", "nc",
           "nd", "oh", "ok", "or", "pa", "ri", "sc", "sd", "tn", "tx", "ut",
           "vt", "va", "wa", "wv", "wi", "wy")

BASE_DAILY_URL = paste0(
      'https://raw.githubusercontent.com/google-research/open-covid-19-data/',
      'master/data/exports/search_trends_symptoms_dataset/',
      'United%20States%20of%20America/subregions/{state}/',
      '2020_US_{state_underscore}_daily_symptoms_dataset.csv')
cache_data_list = list()
signal_description_df = tribble(
    ~signal,            ~description,
    'Podalgia',                         'pain in the foot',
    'Anosmia',                          'loss of smell',
    'Purpura',                          "red/purple skin spots; 'blood spots'",
    'Radiculopathy',                    'pinched nerve',
    'Ageusia',                          'loss of taste',
    'Erythema chronicum migrans',       'expanding rash early in lyme disease',
    'Photodermatitis',                  'allergic rash that reqs light',
)
```

```{r declare_helper_functions}
expand_state_name = function(state) {
  state_name = POSTAL_TO_STATE[[str_to_upper(state)]]
  return(state_name)
}

load_state_data = function(state) {
  if (state %in% names(cache_data_list)) return (cache_data_list[[state]])
  # Check whether there is a cached version
  state_fname = sprintf('cache/%s.csv', state)
  # if there isn't, then download
  if (!file.exists(state_fname)) {
    state_name = expand_state_name(state)
    message(sprintf('Downloading data for %s...', state_name))
    state_name_underscore = str_replace_all(state_name, ' ', '_')
    STATE_DAILY_URL = str_replace_all(BASE_DAILY_URL,
                                   fixed('{state}'), state_name)
    STATE_DAILY_URL = str_replace_all(STATE_DAILY_URL,
                                   fixed('{state_underscore}'),
                                   state_name_underscore)
    STATE_DAILY_URL = str_replace_all(STATE_DAILY_URL,
                                   fixed(' '),
                                   '%20')
    download.file(STATE_DAILY_URL, state_fname)
  }
  single_state = readr::read_csv(state_fname)
  cache_data_list[[state]] <<- single_state
  return (single_state)
}


pull_data_state = function(state, symptom) {
  single_state = load_state_data(state)
  unique(single_state$sub_region_2_code)
  single_state_counties = single_state[!is.na(single_state$sub_region_2_code),]
  selected_symptom = paste0('symptom:', symptom)
  single_state_symptom = single_state_counties[,c('sub_region_2_code',
                                                  'date',
                                                  selected_symptom)]
  # Shape into what we want
  colnames(single_state_symptom) = c('geo_value', 'time_value', 'value')
  single_state_symptom = single_state_symptom %>% filter (
      !is.na(value),
    )
  single_state_symptom = single_state_symptom %>% transmute (
      geo_value = sprintf('%05d', as.numeric(geo_value)),
      signal = symptom,
      time_value = time_value,
      direction = NA,
      issue = lubridate::today(),
      lag = issue - time_value,
      value = value,
      stderr = NA,
      sample_size = NA,
      data_source = 'google_symptoms',
    )
}
```

```{r read_google_symptoms_data, class.source = 'fold-show', message=FALSE, warnings=FALSE}
dir.create('./cache/')
ak = load_state_data('ak')
symptom_cols = colnames(ak)[
                  str_detect(colnames(ak), 'symptom:')]
symptom_names = str_replace(symptom_cols, fixed('symptom:'), '')

if (file.exists('symptom_df.RDS')) {
  symptom_df = readRDS('symptom_df.RDS')
} else {
  symptom_df_list = vector('list', length(symptom_names))
  names(symptom_df_list) = symptom_names

  for (symptom in symptom_names) {
    cat(symptom, '...\n')
    states_list = vector('list', length(states))
    for (idx in 1:length(states)) {
      state = states[idx]
      states_list[[idx]] = pull_data_state(state, symptom)
    }
    symptom_df_list[[symptom]] = bind_rows(states_list)
  }
  symptom_df = bind_rows(symptom_df_list)
  saveRDS(symptom_df, 'symptom_df.RDS')
}
```


```{r download_filter_data, echo = TRUE, message=FALSE, warnings=FALSE, cache=TRUE}
start_day = "2020-03-01"
end_day = "2020-08-15"

df_inum = covidcast_signal(data_source = "jhu-csse",
                   signal = "confirmed_7dav_incidence_num",
                   start_day = start_day, end_day = end_day)

case_num = 500
geo_values = df_inum %>% group_by(geo_value) %>%
  summarize(total = sum(value)) %>%
  filter(total >= case_num) %>% pull(geo_value)
df_inum_act = df_inum %>% filter(geo_value %in% geo_values)
```
```{r subset_symptom_data, echo = TRUE}
symptom_df_act = symptom_df %>% filter (
  geo_value %in% geo_values,
)
```

Here we plot the availaibility of each symptom over time
(proportion is percentage of counties for which the symptom
was available).  We see that for each signal, the availability
level is consistent over time, subject to a strong weekend
effect.

```{r assess_missingness, echo = TRUE}
availability_df = symptom_df_act %>% group_by (
  time_value,
  signal,
) %>% summarize (
  prop_available = n() / length(geo_values),
) %>% ungroup (
)

plt = (ggplot(availability_df)
       + geom_line(aes(x=time_value,
                       y=prop_available,
                       group=factor(signal)),
                   color='dodgerblue4',
                   size=0.1)
       )
plt
```

The symptoms for which data is most sparse are:

```{r symptoms_most_missingness, echo = TRUE}
most_missing = availability_df %>% group_by (
  signal,
) %>% summarize (
  avg_available = mean(prop_available)
) %>% ungroup (
) %>% filter (
  avg_available <= 0.05
) %>% arrange (
  avg_available,
)
```

For the signal that is most sparsely available, the number of 
counties at which it tends to be available daily is:

```{r symptom_min_counties_available, echo = TRUE}
min(most_missing$avg_available) * length(geo_values)
```

Based on this, we leave all the symptoms in for the full
correlations analysis.

```{r calculate_correlations, echo = TRUE}
cor_list = vector('list', length(symptom_names))
names(cor_list) = symptom_names

if (file.exists('cor_df.RDS')) {
  cor_df = readRDS('cor_df.RDS')
} else {
  for (symptom in symptom_names) {
    cat(symptom, '...\n')
    df_cor1 = covidcast_cor(symptom_df_act %>% filter(signal == symptom),
                            df_inum_act,
                            by = "time_value",
                            method = "spearman")
    df_cor1['signal'] = symptom
    cor_list[[symptom]] = df_cor1
  }
  cor_df = bind_rows(cor_list)
  saveRDS(cor_df, 'cor_df.RDS')
}
cor_df = cor_df %>% left_join(
  signal_description_df,
  on='signal',
)
```

```{r correlation_demo, echo = FALSE}
min_available_time = cor_df %>% filter(
    !is.na(value),
  ) %>% pull (
    time_value,
  ) %>% min
plot_cor_df = cor_df %>% filter(time_value >= min_available_time)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   ,
                   color='dodgerblue4',
                   size=0.1,
                   alpha=1.0)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       )
plt
```

```{r investigate_max_max_cor, echo = TRUE}
top_cor_signals = cor_df %>% group_by (
    signal,
  ) %>% filter (
    abs(value) == max(abs(value), na.rm=TRUE),
  ) %>% ungroup (
  ) %>% arrange(
    -abs(value),
  ) %>% head (
    5,
  )
print('Symptoms with the largest all-time correlation:')
print(top_cor_df)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   data=plot_cor_df %>% filter (
                      !signal %in% top_cor_signals$signal
                   ),
                   color='cornsilk',
                   size=0.1,
                   alpha=1.0)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal),
                       colour=factor(signal)
                       ),
                   data=plot_cor_df %>% filter (
                      signal %in% top_cor_signals$signal,
                   ),
                   #color='darkorange',
                   size=0.3)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       + ggtitle("Top 5 signals by all-time max(|corr|)")
       )
plt
```

```{r investigate_max_min_cor, echo = FALSE}
top_min_cor = cor_df %>% group_by (
    signal,
  ) %>% filter (
    abs(value) == min(abs(value), na.rm=TRUE),
  ) %>% ungroup (
  ) %>% arrange(
    -abs(value),
  ) %>% head (
    5,
  )
print('Symptoms that consistently stay away from 0 correlation:')
print(top_min_cor)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   data=plot_cor_df %>% filter (
                      !signal %in% top_min_cor$signal
                   ),
                   color='cornsilk',
                   size=0.1,
                   alpha=1.0)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal),
                       color=factor(signal)),
                   data=plot_cor_df %>% filter (
                      signal %in% top_min_cor$signal,
                   ),
                   size=0.3)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       + ggtitle("Top 5 signals by all-time min(|corr|)")
       )
plt
print(top_min_cor$signal)
print(intersect(top_min_cor$signal, top_cor_signals$signal[1:5]))
```

Could the negative correlation at the beginning of the pandemic be due to:

1. Social distancing / lockdown => non-critical medical visits cancelled
2. Fewer non-COVID diagnoses => fewer unfamiliar terms for people to 
   research on Google
3. We could examine this hypothesis more closely by correlating these
   "null" symptoms against mobility / Safegraph data for medical building
   visits (not to be confused with the Doctor's Visits signal, which
   specifically counts doctor's visits for CLI).


TODO:

* [X] Re-create overall correlations plot
* [X] Look at the "consistently ok" ones - top 5 min
* [ ] Table that lists symptom, summary statistics of correlation (transform
      existing table.......
* [ ] Top 5: Group by location, aggregate over time, run correlation
* [ ] With county-sliced correlations, produce chloropeth
  layperson description.  Then plot using:
  https://cmu-delphi.github.io/covidcast/covidcastR/articles/correlation-utils.html
* [ ] Run lasso / quantile lasso fit
  * [ ] One fit that uses my "curated" variables
  * [ ] One fit that just throws everything in
* [ ] Polish and add some prose


### Appendix
Some things I'm still worried about / next steps:

* Efficient way to present data?  Distribution of correlations over time,
  and the symptoms that consistently achieved high correlations
* For those very useful symptoms, is the correlation over time homogeneous 
  in geography, or not?  If not homogeneous in geography, we may want to 
  be careful with fitting a global model; we don't want to inadvertently
  make poor predictions for some places.
