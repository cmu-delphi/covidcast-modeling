---
title: "Google Symptoms Dataset"
author: "Addison"
output:
  html_document:
    toc: true
    code_folding: hide
---

```{r import_statements, echo = FALSE, message = FALSE}
library(tidyverse)
library(covidcast)
library(dplyr)
library(ggplot2)
```

```{r declare_global_constants}
POSTAL_TO_STATE = list('AL'='Alabama', 'AK'='Alaska', 'AS'='American Samoa',
                       'AZ'='Arizona', 'AR'='Arkansas', 'CA'='California',
                       'CO'='Colorado', 'CT'='Connecticut', 'DE'='Delaware',
                       'DC'='District of Columbia', 'FL'='Florida',
                       'GA'='Georgia', 'GU'='Guam', 'HI'='Hawaii',
                       'ID'='Idaho', 'IL'='Illinois', 'IN'='Indiana',
                       'IA'='Iowa', 'KS'='Kansas', 'KY'='Kentucky',
                       'LA'='Louisiana', 'ME'='Maine', 'MD'='Maryland',
                       'MA'='Massachusetts', 'MI'='Michigan', 'MN'='Minnesota',
                       'MS'='Mississippi', 'MO'='Missouri', 'MT'='Montana',
                       'NE'='Nebraska', 'NV'='Nevada', 'NH'='New Hampshire',
                       'NJ'='New Jersey', 'NM'='New Mexico', 'NY'='New York',
                       'NC'='North Carolina', 'ND'='North Dakota',
                       'MP'='Northern Mariana Islands', 'OH'='Ohio',
                       'OK'='Oklahoma', 'OR'='Oregon', 'PA'='Pennsylvania',
                       'PR'='Puerto Rico', 'RI'='Rhode Island', 'SC'='South Carolina',
                       'SD'='South Dakota', 'TN'='Tennessee',
                       'TX'='Texas', 'UT'='Utah', 'VT'='Vermont', 'VI'='Virgin Islands',
                       'VA'='Virginia', 'WA'='Washington', 'WV'='West Virginia',
                       'WI'='Wisconsin', 'WY'='Wyoming')

states = c("al", "ak", "az", "ar", "ca", "co", "ct", "de", "fl", "ga", "hi",
           "id", "il", "in", "ia", "ks", "ky", "la", "me", "md", "ma", "mi",
           "mn", "ms", "mo", "mt", "ne", "nv", "nh", "nj", "nm", "ny", "nc",
           "nd", "oh", "ok", "or", "pa", "ri", "sc", "sd", "tn", "tx", "ut",
           "vt", "va", "wa", "wv", "wi", "wy")

BASE_DAILY_URL = paste0(
      'https://raw.githubusercontent.com/google-research/open-covid-19-data/',
      'master/data/exports/search_trends_symptoms_dataset/',
      'United%20States%20of%20America/subregions/{state}/',
      '2020_US_{state_underscore}_daily_symptoms_dataset.csv')
cache_data_list = list()
signal_description_df = tribble(
    ~signal,            ~description,
    'Podalgia',                         'pain in the foot',
    'Anosmia',                          'loss of smell',
    'Purpura',                          "red/purple skin spots; 'blood spots'",
    'Radiculopathy',                    'pinched nerve',
    'Ageusia',                          'loss of taste',
    'Erythema chronicum migrans',       'expanding rash early in lyme disease',
    'Photodermatitis',                  'allergic rash that reqs light',
)
```

```{r declare_helper_functions}
expand_state_name = function(state) {
  state_name = POSTAL_TO_STATE[[str_to_upper(state)]]
  return(state_name)
}

load_state_data = function(state) {
  if (state %in% names(cache_data_list)) return (cache_data_list[[state]])
  # Check whether there is a cached version
  state_fname = sprintf('cache/%s.csv', state)
  # if there isn't, then download
  if (!file.exists(state_fname)) {
    state_name = expand_state_name(state)
    message(sprintf('Downloading data for %s...', state_name))
    state_name_underscore = str_replace_all(state_name, ' ', '_')
    STATE_DAILY_URL = str_replace_all(BASE_DAILY_URL,
                                   fixed('{state}'), state_name)
    STATE_DAILY_URL = str_replace_all(STATE_DAILY_URL,
                                   fixed('{state_underscore}'),
                                   state_name_underscore)
    STATE_DAILY_URL = str_replace_all(STATE_DAILY_URL,
                                   fixed(' '),
                                   '%20')
    download.file(STATE_DAILY_URL, state_fname)
  }
  single_state = readr::read_csv(state_fname)
  cache_data_list[[state]] <<- single_state
  return (single_state)
}


pull_data_state = function(state, symptom) {
  single_state = load_state_data(state)
  unique(single_state$sub_region_2_code)
  single_state_counties = single_state[!is.na(single_state$sub_region_2_code),]
  selected_symptom = paste0('symptom:', symptom)
  single_state_symptom = single_state_counties[,c('sub_region_2_code',
                                                  'date',
                                                  selected_symptom)]
  # Shape into what we want
  colnames(single_state_symptom) = c('geo_value', 'time_value', 'value')
  single_state_symptom = single_state_symptom %>% filter (
      !is.na(value),
    )
  single_state_symptom = single_state_symptom %>% transmute (
      geo_value = sprintf('%05d', as.numeric(geo_value)),
      signal = symptom,
      time_value = time_value,
      direction = NA,
      issue = lubridate::today(),
      lag = issue - time_value,
      value = value,
      stderr = NA,
      sample_size = NA,
      data_source = 'google_symptoms',
    )
}
```

### Summary

* 422 symptom signals, each with varying levels of availability.  The most
  sparsely available symptoms are still available at at least XXXX counties
  daily.  As a result, we retain all symptoms for this initial round of
  analysis.
* *First*, we assess the correlation between each symptom and cases over
  time.  To do this, we group by (symptom, date), and calculate the rank
  correlation between search interest and cases (the free variable here
  being counties).  (A previous analysis found that leading / lagging the
  signal does not improve correlation, which is in line with the intuition
  that searcher interest should provide a "today" snapshot of COVID
  prevalence).
* We see that there are a handful of symptoms that are highly correlated
  with cases, but there is a surprising temporal dependency.  Specifically,
  almost all signals suffer a degradation in correlation from XXXX to
  XXXX.  This is likely due to non-stationarity in both searcher behavior as
  well as in the dynamics of COVID-spread (which is also highly community
  dependent).
* The symptoms that achieve the greatest single-day correlation with cases
  include the usual suspects: anosmia (lack of smell), ageusia (lack of taste),
  fever, XXXX, XXXX.  Concerningly, these high-signal symptoms also
  suffer from degradation during XXXX, with anosmia bottoming out with a
  near-zero correlation with cases on XXXX.  We need to develop an
  understanding of why even these correlations degrade.  Nonetheless,
  these symptoms will likely form the bedrock of a new Google Symptoms
  indicator.
* We also examine the symptoms which maximize the minimum absolute correlation
  -- i.e., the symptoms that are uniformly bounded away from zero (many
  symptoms that achieve high single-day correlations also have days 
  with very low correlation).  These symptoms are not medically linked
  to the coronavirus, based on my own rudimentary understanding.  However,
  we pose a latent variable that may explain why they achieve consistent
  negative correlation with cases: more cases in a community discourages
  non-vital medical appointments, decreasing diagnoses which may spur
  patients to Google unfamiliar terms like "Erythema chronicum migrans".
* The hypothesis that an increase in cases reduces search interest in
  medical terms, especially obscure ones, by reducing non-vital medical
  visits may be further examined by incorporating Safegraph location data
  for general medical visits (not to be confused with the Doctor's Visits
  indicator, which measures the proportion of doctor's visits that are
  related to COVID).
* *Second*, we assess whether the correlation between search interest in
  these symptoms and cases is homogeneous in location.  To do this,
  we group by (symptom, county), and calculate the correlation varying time.
* TODO
* *Finally*, we set up a rudimentary prediction problem, modeled on
  XXXX, to assess the forecasting ability of this new data alone.
* TODO

### Initial ingestion and exploration

```{r read_google_symptoms_data, class.source = 'fold-show', message=FALSE, warnings=FALSE}
if (file.exists('symptom_df.RDS')) {
  symptom_df = readRDS('symptom_df.RDS')
  symptom_names = unique(symptom_df$signal)
} else {
  dir.create('./cache/')
  ak = load_state_data('ak')
  symptom_cols = colnames(ak)[
                    str_detect(colnames(ak), 'symptom:')]
  symptom_names = str_replace(symptom_cols, fixed('symptom:'), '')

  symptom_df_list = vector('list', length(symptom_names))
  names(symptom_df_list) = symptom_names

  for (symptom in symptom_names) {
    cat(symptom, '...\n')
    states_list = vector('list', length(states))
    for (idx in 1:length(states)) {
      state = states[idx]
      states_list[[idx]] = pull_data_state(state, symptom)
    }
    symptom_df_list[[symptom]] = bind_rows(states_list)
  }
  symptom_df = bind_rows(symptom_df_list)
  saveRDS(symptom_df, 'symptom_df.RDS')
}
```


```{r download_filter_data, echo = TRUE, message=FALSE, warnings=FALSE, cache=TRUE}
start_day = "2020-03-01"
end_day = "2020-08-15"

df_inum = covidcast_signal(data_source = "jhu-csse",
                   signal = "confirmed_7dav_incidence_num",
                   start_day = start_day, end_day = end_day)

case_num = 500
geo_values = df_inum %>% group_by(geo_value) %>%
  summarize(total = sum(value)) %>%
  filter(total >= case_num) %>% pull(geo_value)
df_inum_act = df_inum %>% filter(geo_value %in% geo_values)
```
```{r subset_symptom_data, echo = TRUE}
symptom_df_act = symptom_df %>% filter (
  geo_value %in% geo_values,
)
```

Here we plot the availaibility of each symptom over time
(proportion is percentage of counties for which the symptom
was available).  We see that for each signal, the availability
level is consistent over time, subject to a strong weekend
effect.

```{r assess_missingness, echo = TRUE}
availability_df = symptom_df_act %>% group_by (
  time_value,
  signal,
) %>% summarize (
  prop_available = n() / length(geo_values),
) %>% ungroup (
)

plt = (ggplot(availability_df)
       + geom_line(aes(x=time_value,
                       y=prop_available,
                       group=factor(signal)),
                   color='dodgerblue4',
                   size=0.1)
       )
plt
```

The symptoms for which data is most sparse are:

```{r symptoms_most_missingness, echo = TRUE}
most_missing = availability_df %>% group_by (
  signal,
) %>% summarize (
  avg_available = mean(prop_available)
) %>% ungroup (
) %>% filter (
  avg_available <= 0.05
) %>% arrange (
  avg_available,
)
print(most_missing)
```

For the signal that is most sparsely available, the number of 
counties at which it tends to be available daily is:

```{r symptom_min_counties_available, echo = TRUE}
print(min(most_missing$avg_available) * length(geo_values))
```

Based on this, we leave all the symptoms in for the full
correlations analysis.

### Correlations 

```{r calculate_correlations, echo = TRUE}
cor_list = vector('list', length(symptom_names))
names(cor_list) = symptom_names

if (file.exists('cor_df.RDS')) {
  cor_df = readRDS('cor_df.RDS')
} else {
  for (symptom in symptom_names) {
    cat(symptom, '...\n')
    df_cor1 = covidcast_cor(symptom_df_act %>% filter(signal == symptom),
                            df_inum_act,
                            by = "time_value",
                            method = "spearman")
    df_cor1['signal'] = symptom
    cor_list[[symptom]] = df_cor1
  }
  cor_df = bind_rows(cor_list)
  saveRDS(cor_df, 'cor_df.RDS')
}
cor_df = cor_df %>% left_join(
  signal_description_df,
  on='signal',
)
```

#### Correlation over time: all symptoms
```{r correlation_demo, echo = FALSE}
min_available_time = cor_df %>% filter(
    !is.na(value),
  ) %>% pull (
    time_value,
  ) %>% min
plot_cor_df = cor_df %>% filter(time_value >= min_available_time)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   ,
                   color='dodgerblue4',
                   size=0.1,
                   alpha=1.0)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       )
plt
```

#### Correlation over time: largest single-day correlation
When we discuss the "size" of a correlation, we consider the absolute value
of correlation.

```{r investigate_max_max_cor, echo = TRUE}
top_cor_signals = plot_cor_df %>% group_by (
    signal,
  ) %>% filter (
    abs(value) == max(abs(value), na.rm=TRUE),
  ) %>% ungroup (
  ) %>% arrange(
    -abs(value),
  ) %>% head (
    5,
  )
top_cor_sum_stats = plot_cor_df %>% filter (
    signal %in% top_cor_signals$signal,
  ) %>% group_by (
    signal,
  ) %>% summarize (
    min = min(value, na.rm=TRUE),
    quart1 = quantile(value, 0.25, na.rm=TRUE),
    med = median(value, na.rm=TRUE),
    mean = mean(value, na.rm=TRUE),
    quart3 = quantile(value, 0.75, na.rm=TRUE),
    max = max(value, na.rm=TRUE),
  ) %>% ungroup (
  )
print('Symptoms with the largest all-time correlation:')
print(top_cor_signals %>% left_join(top_cor_sum_stats, on='signal')
        %>% select(-time_value, -value),
      width=100)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   data=plot_cor_df %>% filter (
                      !signal %in% top_cor_signals$signal
                   ),
                   color='cornsilk',
                   size=0.1,
                   alpha=1.0)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal),
                       colour=factor(signal)
                       ),
                   data=plot_cor_df %>% filter (
                      signal %in% top_cor_signals$signal,
                   ),
                   #color='darkorange',
                   size=0.3)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       + ggtitle("Top 5 signals by all-time max(|corr|)")
       )
plt
```

#### Correlation over time: "consistently away from zero" symptoms

```{r investigate_max_min_cor, echo = FALSE}
top_min_cor = plot_cor_df %>% group_by (
    signal,
  ) %>% filter (
    abs(value) == min(abs(value), na.rm=TRUE),
  ) %>% ungroup (
  ) %>% arrange(
    -abs(value),
  ) %>% head (
    5,
  )
top_min_sum_stats = plot_cor_df %>% filter (
    signal %in% top_min_cor$signal,
  ) %>% group_by (
    signal,
  ) %>% summarize (
    min = min(value, na.rm=TRUE),
    quart1 = quantile(value, 0.25, na.rm=TRUE),
    med = median(value, na.rm=TRUE),
    mean = mean(value, na.rm=TRUE),
    quart3 = quantile(value, 0.75, na.rm=TRUE),
    max = max(value, na.rm=TRUE),
  ) %>% ungroup (
  )
print('Symptoms that consistently stay away from 0 correlation:')
print(top_min_cor %>% left_join(top_min_sum_stats, on='signal')
        %>% select(-time_value, -value),
      width=100)
plt = (ggplot(plot_cor_df)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal)),
                   data=plot_cor_df %>% filter (
                      !signal %in% top_min_cor$signal
                   ),
                   color='cornsilk',
                   size=0.1,
                   alpha=1.0)
       + geom_line(aes(x=time_value,
                       y=value,
                       group=factor(signal),
                       color=factor(signal)),
                   data=plot_cor_df %>% filter (
                      signal %in% top_min_cor$signal,
                   ),
                   size=0.3)
       + ylab('rank correlation')
       + scale_x_date(breaks=lubridate::ymd(c('2020-03-01',
            '2020-03-15', '2020-04-01', '2020-04-15', '2020-05-01',
            '2020-05-15', '2020-06-01', '2020-06-15', '2020-07-01',
            '2020-07-15', '2020-08-01', '2020-08-15')))
       + theme(axis.text.x = element_text(angle = 45))
       + ggtitle("Top 5 signals by all-time min(|corr|)")
       )
plt
print(top_min_cor$signal)
print(intersect(top_min_cor$signal, top_cor_signals$signal[1:5]))
```
Interestingly, although anosmia technically is one of the "most stable" by our
criterion, its distribution actually covers zero.

### Correlation across location: TODO

```{r calculate_correlations_geo, echo = TRUE}
if (file.exists('geo_cor_df.RDS')) {
  geo_cor_df = readRDS('geo_cor_df.RDS')
} else {
  geo_cor_list = vector('list', length(symptom_names))
  names(geo_cor_list) = symptom_names

  for (symptom in symptom_names) {
    cat(symptom, '...\n')
    df_cor1 = covidcast_cor(symptom_df_act %>% filter(signal == symptom),
                            df_inum_act,
                            by = "geo_value",
                            method = "spearman")
    df_cor1['signal'] = symptom
    geo_cor_list[[symptom]] = df_cor1
  }
  geo_cor_df = bind_rows(geo_cor_list)
  saveRDS(geo_cor_df, 'geo_cor_df.RDS')
}
geo_cor_df = geo_cor_df %>% left_join(
  signal_description_df,
  on='signal',
)
```

```{r plot_chloropeth_max_cor, echo = TRUE}
for (symptom in top_cor_signals$signal) {
  df_cor2 = geo_cor_df %>% filter (signal == symptom)
  df_cor2$time_value = min_available_time
  df_cor2$issue = min_available_time
  attributes(df_cor2)$geo_type = 'county'
  class(df_cor2) = c("covidcast_signal", "data.frame")
  n_available_county = df_cor2 %>% filter (!is.na(value)) %>% nrow()

  # Plot choropleth maps, using the covidcast plotting functionality
  title_text = sprintf("Correlations between cases and %s (%d counties)",
                             symptom, n_available_county)
  if (!is.na(df_cor2$description[1])) {
    title_text = paste0(title_text, '\n', sprintf('(%s)', df_cor2$description[1]))
  } 
  print(plot(df_cor2,
             title = title_text,
            range = c(-1, 1), choro_col = c("orange","lightblue", "purple")))
}
```

```{r plot_chloropeth_min_cor, echo = TRUE}
for (symptom in top_min_cor$signal) {
  df_cor2 = geo_cor_df %>% filter (signal == symptom)
  df_cor2$time_value = min_available_time
  df_cor2$issue = min_available_time
  attributes(df_cor2)$geo_type = 'county'
  class(df_cor2) = c("covidcast_signal", "data.frame")
  n_available_county = df_cor2 %>% filter (!is.na(value)) %>% nrow()

  # Plot choropleth maps, using the covidcast plotting functionality
  title_text = sprintf("Correlations between cases and %s (%d counties)",
                             symptom, n_available_county)
  if (!is.na(df_cor2$description[1])) {
    title_text = paste0(title_text, '\n', sprintf('(%s)', df_cor2$description[1]))
  } 
  print(plot(df_cor2,
             title = title_text,
            range = c(-1, 1), choro_col = c("orange","lightblue", "purple")))
}
```

TODO:

* [X] Re-create overall correlations plot
* [X] Look at the "consistently ok" ones - top 5 min
* [x] Table that lists symptom, summary statistics of correlation (transform
      existing table.......
* [X] Top 5: Group by location, aggregate over time, run correlation
* [ ] With county-sliced correlations, produce chloropeth for top 5
  layperson description.  Then plot using:
  https://cmu-delphi.github.io/covidcast/covidcastR/articles/correlation-utils.html
* [ ] Run lasso / quantile lasso fit
  * [ ] One fit that uses my "curated" variables
  * [ ] One fit that just throws everything in
* [ ] Polish and add some prose


### Appendix
Some things I'm still worried about / next steps:

* Efficient way to present data?  Distribution of correlations over time,
  and the symptoms that consistently achieved high correlations
* For those very useful symptoms, is the correlation over time homogeneous 
  in geography, or not?  If not homogeneous in geography, we may want to 
  be careful with fitting a global model; we don't want to inadvertently
  make poor predictions for some places.
